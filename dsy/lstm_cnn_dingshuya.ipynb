{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder-Decoder structures are widely used in seq2seq model and other applications.\n",
    "\n",
    "Beyond that, CNN has the most outstanding results in image classifications\n",
    "\n",
    "We hypothesis that RNN-CNN network can deal with incomplete sketches better as this structure take consider the stroke sequence of sketches. Meanwhile, CNN is used for predicting the labels using hidden feature vectors provided by RNN encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/heylamourding/miniconda3/bin/python'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4fcfd618f83c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_growth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "import glob\n",
    "import os.path as path\n",
    "import ndjson\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our team will use one preprocessed data raw files for modelling in order to compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_data import *\n",
    "dataset_path='/home/heylamourding/quickdraw/dsy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,train_Y,test_X,test_Y=get_dataset(dataset_path,test_r=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_trainX = pd.DataFrame({'drawing':train_X})\n",
    "df_testX = pd.DataFrame({'drawing':test_X})\n",
    "# Calculate Stroke Number of Each Image\n",
    "df_trainX['stroke_number'] = df_trainX['drawing'].str.len()\n",
    "df_testX['stroke_number'] = df_testX['drawing'].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use datasets format which proposed in sketch RNN. Each example in the dataset is stored as list of coordinate offsets: ∆x, ∆y, and a binary value representing whether the pen is lifted away from the paper. \n",
    "\n",
    "In this notebook, we will try to use two type of structures. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 1st Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension = (samples) * (3* max points of each strokes of all drawings) * (max strokes of all drawings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def create_stroke(df):\n",
    "    final = []\n",
    "    # Image\n",
    "    for i in range(df.shape[0]):\n",
    "        num = df.loc[i,'stroke_number']\n",
    "        # Strokes \n",
    "        stroke_ls = []\n",
    "        for stroke in range(num):\n",
    "            X = df.loc[i,'drawing'][stroke][0] #points of stroke \n",
    "            Y = df.loc[i,'drawing'][stroke][1] #points of stroke\n",
    "            X_offset = np.diff(np.array(X)) # points of stroke \n",
    "            Y_offset = np.diff(np.array(Y)) # points of stroke\n",
    "            binary = [0]*(X_offset.shape[0]-1)\n",
    "            binary.append(1) # points of stroke \n",
    "            binary = np.array(binary)\n",
    "            stroke_ar = np.vstack((X_offset,Y_offset,binary)).reshape(-1) \n",
    "            stroke_ls.append(stroke_ar)\n",
    "        final.append(stroke_ls)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_final = create_stroke(df_trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max stroke is:  stroke_number    30\n",
      "dtype: int64\n",
      "Min stroke is:  stroke_number    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Max stroke is: ', df_trainX[['stroke_number']].max())\n",
    "print('Min stroke is: ', df_trainX[['stroke_number']].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max dimensions *3 of overall stroke 597\n",
      "Min dimensions *3 of overall stroke 3\n"
     ]
    }
   ],
   "source": [
    "max_stroke = 0 \n",
    "min_stroke = 500\n",
    "for i in range(len(trainX_final)):\n",
    "    for j in range(len(trainX_final[i])):\n",
    "        temp = trainX_final[i][j].shape[0]\n",
    "        if temp < min_stroke:\n",
    "            min_stroke = temp\n",
    "        elif temp > max_stroke:\n",
    "            max_stroke = temp\n",
    "print('Max dimensions *3 of overall stroke', max_stroke) \n",
    "print('Min dimensions *3 of overall stroke', min_stroke)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As what mentioned above, I will preprocess trainX_final into following dimensions **N\\*597\\*30**\n",
    "\n",
    "Before this, as our computational resources is limited, only part of training data will be used. In order to eliminate imbalanced data effect, I will use following functions to select balanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Balanced Data filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "def balance_filter(ls, label_list, name, SL, EL):   \n",
    "    # Get indices list that have balance labels \n",
    "    label_ar = np.array(label_list)\n",
    "    label1_indices = np.array(np.where(label_ar == 0)).reshape(-1)[SL:EL].tolist()\n",
    "    label2_indices = np.array(np.where(label_ar == 1)).reshape(-1)[SL:EL].tolist()\n",
    "    label3_indices = np.array(np.where(label_ar == 2)).reshape(-1)[SL:EL].tolist()\n",
    "    label4_indices = np.array(np.where(label_ar == 3)).reshape(-1)[SL:EL].tolist()\n",
    "    label5_indices = np.array(np.where(label_ar == 4)).reshape(-1)[SL:EL].tolist()\n",
    "    train_indices = []\n",
    "    temp = [label1_indices, label2_indices, label3_indices, label4_indices, label5_indices]\n",
    "    for i in range(5):\n",
    "        target = temp[i]\n",
    "        for item in range(len(target)):\n",
    "            train_indices.append(target[item])\n",
    "#     print(train_indices)\n",
    "    # Shuffle indices\n",
    "    shuffle(train_indices)\n",
    "    # Select X & Y\n",
    "    final_X_ls = []\n",
    "    final_label_ls = []\n",
    "    for i in range(len(train_indices)):\n",
    "#         print('i')\n",
    "#         print(i)\n",
    "        indice = train_indices[i]\n",
    "        #print('indice')\n",
    "        #print(indice)\n",
    "        final_X_ls.append(ls[indice])\n",
    "        final_label_ls.append(label_list[indice])\n",
    "\n",
    "    final_label_ar = np.array(final_label_ls)\n",
    "    print(name +'label 0 has', sum(final_label_ar==0))\n",
    "    print(name +'label 1 has', sum(final_label_ar==1))\n",
    "    print(name +'label 2 has', sum(final_label_ar==2))\n",
    "    print(name +'label 3 has', sum(final_label_ar==3))\n",
    "    print(name + 'label 4 has', sum(final_label_ar==4))\n",
    "\n",
    "    return final_X_ls, final_label_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainlabel 0 has 3000\n",
      "trainlabel 1 has 3000\n",
      "trainlabel 2 has 3000\n",
      "trainlabel 3 has 3000\n",
      "trainlabel 4 has 3000\n"
     ]
    }
   ],
   "source": [
    "train_X_balance, train_Y_balance = balance_filter(trainX_final, train_Y, 'train', 0, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max dimensions *3 of overall stroke 597\n",
      "Min dimensions *3 of overall stroke 3\n"
     ]
    }
   ],
   "source": [
    "max_stroke = 0 \n",
    "min_stroke = 500\n",
    "for i in range(len(train_X_balance)):\n",
    "    for j in range(len(train_X_balance[i])):\n",
    "        temp = train_X_balance[i][j].shape[0]\n",
    "        if temp < min_stroke:\n",
    "            min_stroke = temp\n",
    "        elif temp > max_stroke:\n",
    "            max_stroke = temp\n",
    "print('Max dimensions *3 of overall stroke', max_stroke) \n",
    "print('Min dimensions *3 of overall stroke', min_stroke)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Padding Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order simplifying the RNN training process, we will padding our training data into same time steps length and same dimensions. \n",
    "\n",
    "It means that two operations will be done. One is padding points of each stroke of all samples to 597 and padding strokes of each images to 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_points(data_stroke, max_pts = 597):\n",
    "    final = []\n",
    "    for i in range(len(data_stroke)):\n",
    "        # ith image \n",
    "        for j in range(len(data_stroke[i])):\n",
    "            # jth strokes \n",
    "            orig = len(data_stroke[i][j])\n",
    "            # pts*3 of jth stroke \n",
    "            #print(orig)\n",
    "            if orig < max_pts:\n",
    "                pad = np.array([0]*(max_pts-orig))\n",
    "                data_stroke[i][j] = np.hstack((data_stroke[i][j],pad))\n",
    "            else:\n",
    "                data_stroke[i][j] = data_stroke[i][j][:max_pts]\n",
    "            #print(pad.shape)\n",
    "            #print(data_stroke[i][j].shape)\n",
    "\n",
    "        final.append(np.array(data_stroke[i]))\n",
    "            #print(data_stroke[i][j].shape)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_input = pad_points(train_X_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_stroke(data_stroke, max_stroke, max_pts):\n",
    "    for i in range(len(data_stroke)):\n",
    "        # ith image \n",
    "        # get No.stroke for this image\n",
    "        orig = data_stroke[i].shape[0]\n",
    "        #print(orig)\n",
    "        pad = np.zeros(((max_stroke-orig),max_pts))\n",
    "        #print(pad.shape)\n",
    "        #print(data_stroke[i].shape)\n",
    "        data_stroke[i] = np.vstack((data_stroke[i],pad))\n",
    "        #print(data_stroke[i].shape)\n",
    "    return data_stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_ar = np.array(pad_stroke(trainX_input, max_stroke=30, max_pts=597))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 30, 597)\n"
     ]
    }
   ],
   "source": [
    "print(trainX_ar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "del trainX_input, train_X_balance, trainX_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "X_train = preprocessing.scale(trainX_ar.reshape(trainX_ar.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape back \n",
    "X_train = X_train.reshape((15000,30,597))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 Categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "categorical_labels = to_categorical(train_Y_balance, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(dataset_path+'/X_train.npy',X_train)\n",
    "np.save(dataset_path+'/Y_train.npy',categorical_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e7dbd320fd8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/X_train.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcategorical_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/Y_train.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "X_train = np.load(dataset_path+'/X_train.npy')\n",
    "categorical_labels = np.load(dataset_path+'/Y_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 30, 597)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple Model Modelling \n",
    "\n",
    "Starting with Small Dataset for hyperparameter tuning. \n",
    "\n",
    "We will use adam as our optimizer hence we won't change learning rate but only use default parameters.\n",
    "\n",
    "Following hyperparameters will be tuned:\n",
    "\n",
    "1. Batch Size [32, 64, 256]\n",
    "2. LSTM Units [100,300,500]\n",
    "3. Dropout Rate [0.2, 0.4, 0.5]\n",
    "4. Encoder Dense [256,400, 625, 1024]\n",
    "5. Decoder Dense [16, 32, 64, 128]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.backend' has no attribute 'theano_backend'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-7ae6ae05c8ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheano_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_available_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'keras.backend' has no attribute 'theano_backend'"
     ]
    }
   ],
   "source": [
    "# Set GPU\n",
    "from keras import backend as K\n",
    "K.theano_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KERAS model (explained above)\n",
    "import keras\n",
    "config = tf.ConfigProto( device_count = {'GPU': 0 } ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, GRU, LSTM\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, Reshape\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['LSTMUnits'] = 300\n",
    "args['batch'] = 256\n",
    "args['epochs'] = 1\n",
    "args['dropout'] = 0.2\n",
    "args['len_category'] = 5\n",
    "args['enc_dense'] = 256\n",
    "args['num_filters'] = 1\n",
    "args['kernelS'] = 3\n",
    "args['stride'] = 2\n",
    "args['poolS'] = 2 \n",
    "args['dec_dense'] = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(args['LSTMUnits'],return_sequences=False,input_shape=(30,597)))\n",
    "model.add(Dense(args['enc_dense'], activation='relu'))\n",
    "model.add(Reshape((16,16,1)))\n",
    "model.add(Conv2D(args['num_filters'],args['kernelS'],strides=(args['stride'],args['stride']), activation ='relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(args['poolS'],args['poolS'])))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(args['dec_dense'], activation='relu'))\n",
    "model.add(Dropout(args['dropout']))\n",
    "model.add(Dense(args['len_category'], activation='softmax'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "          optimizer='adam',\n",
    "          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 300)               1077600   \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               77056     \n",
      "_________________________________________________________________\n",
      "reshape_5 (Reshape)          (None, 16, 16, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 1)           10        \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 49)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                1600      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 1,156,431\n",
      "Trainable params: 1,156,431\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heylamourding/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 33s 3ms/step - loss: 0.1409 - acc: 0.3475 - val_loss: 0.1334 - val_acc: 0.3990\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 32s 3ms/step - loss: 0.1233 - acc: 0.4843 - val_loss: 0.1056 - val_acc: 0.5823\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 33s 3ms/step - loss: 0.1071 - acc: 0.5895 - val_loss: 0.1175 - val_acc: 0.5273\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 32s 3ms/step - loss: 0.1085 - acc: 0.5787 - val_loss: 0.1060 - val_acc: 0.5850\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 33s 3ms/step - loss: 0.0953 - acc: 0.6368 - val_loss: 0.0995 - val_acc: 0.6547\n",
      "Epoch 6/10\n",
      " 8896/12000 [=====================>........] - ETA: 7s - loss: 0.0885 - acc: 0.6761"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-9fe173d773f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(X_train, categorical_labels,\n\u001b[1;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           verbose=1,validation_split=0.2)\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m     \u001b[0;31m# and update `feed_dict` to use the new handle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m     \u001b[0mhandle_movers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfeed_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m       \u001b[0mmover\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_handle_mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmover\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, categorical_labels,\n",
    "          batch_size = args['batch'], nb_epoch= args['epochs'], \n",
    "          verbose=1,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
