{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os.path as path\n",
    "import ndjson\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset_path,class_name):\n",
    "    i = 0\n",
    "    train_strokes = None\n",
    "    train_labels = []\n",
    "    valid_labels = []\n",
    "    test_labels = []\n",
    "    for file_path in glob.iglob(path.join(dataset_path,'*.npz')):\n",
    "        label=file_path.split('/')[-1].split('.')[0]\n",
    "        if class_name!=-1 and label not in class_name:\n",
    "            continue\n",
    "        print('readding data from {}'.format(file_path))\n",
    "        i = i+1\n",
    "        data = np.load(file_path, encoding ='latin1')\n",
    "        if train_strokes is None:\n",
    "            train_strokes = data['train']\n",
    "            valid_strokes = data['valid']\n",
    "            test_strokes = data['test']\n",
    "            print('this class has',str(len(data['train'])+len(data['valid'])+len(data['test'])))\n",
    "            train_labels.append([i-1]*data['train'].shape[0])\n",
    "            valid_labels.append([i-1]*data['valid'].shape[0])\n",
    "            test_labels.append([i-1]*data['test'].shape[0])\n",
    "        else: \n",
    "            train_strokes = np.concatenate((train_strokes, data['train']))\n",
    "            valid_strokes = np.concatenate((valid_strokes, data['valid']))\n",
    "            test_strokes = np.concatenate((test_strokes, data['test']))\n",
    "            print('this class has',str(len(data['train'])+len(data['valid'])+len(data['test'])))\n",
    "            train_labels.append([i-1]*data['train'].shape[0])\n",
    "            valid_labels.append([i-1]*data['valid'].shape[0])\n",
    "            test_labels.append([i-1]*data['test'].shape[0])\n",
    "    train_labels = [item for sublist in train_labels for item in sublist]\n",
    "    test_labels = [item for sublist in test_labels for item in sublist]\n",
    "    valid_labels = [item for sublist in valid_labels for item in sublist]\n",
    "            \n",
    "    return train_strokes,valid_strokes,test_strokes,train_labels,valid_labels,test_labels\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path='/home/heylamourding/quickdraw/data/sy_data/quick_draw'\n",
    "class_name = ['calendar', 'snowman', 'penguin', 'blackberry', 'teddy-bear']  # class_name=-1 means all class\n",
    "country_code = ['JP', 'CN', 'DE']  # country_code=-1 means all countries\n",
    "output_path='/home/heylamourding/quickdraw/data/sy_data/quick_draw_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readding data from /home/heylamourding/quickdraw/data/sy_data/quick_draw/teddy-bear.npz\n",
      "this class has 75000\n",
      "readding data from /home/heylamourding/quickdraw/data/sy_data/quick_draw/blackberry.npz\n",
      "this class has 75000\n",
      "readding data from /home/heylamourding/quickdraw/data/sy_data/quick_draw/penguin.npz\n",
      "this class has 75000\n",
      "readding data from /home/heylamourding/quickdraw/data/sy_data/quick_draw/snowman.npz\n",
      "this class has 75000\n",
      "readding data from /home/heylamourding/quickdraw/data/sy_data/quick_draw/calendar.npz\n",
      "this class has 75000\n"
     ]
    }
   ],
   "source": [
    "train_strokes,valid_strokes,test_strokes,train_labels,valid_labels,test_labels = preprocess(dataset_path,class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max(train_strokes, valid_stroke, test_strokes):\n",
    "    max_stroke = 0\n",
    "    for i in range(train_strokes.shape[0]):\n",
    "        temp = train_strokes[i].shape[0]\n",
    "        if temp >= max_stroke:\n",
    "            max_stroke = temp\n",
    "\n",
    "    for i in range(valid_strokes.shape[0]):\n",
    "        temp = valid_strokes[i].shape[0]\n",
    "        if temp >= max_stroke:\n",
    "            max_stroke = temp\n",
    "    \n",
    "    for i in range(test_strokes.shape[0]):\n",
    "        temp = test_strokes[i].shape[0]\n",
    "        if temp >= max_stroke:\n",
    "            max_stroke = temp\n",
    "    return max_stroke\n",
    "\n",
    "def find_min(train_strokes, valid_strokes, test_strokes):\n",
    "    min_stroke = 100\n",
    "    for i in range(train_strokes.shape[0]):\n",
    "        temp = train_strokes[i].shape[0]\n",
    "        if temp <= min_stroke:\n",
    "            min_stroke = temp\n",
    "\n",
    "    for i in range(valid_strokes.shape[0]):\n",
    "        temp = valid_strokes[i].shape[0]\n",
    "        if temp <= min_stroke:\n",
    "            min_stroke = temp\n",
    "    \n",
    "    for i in range(test_strokes.shape[0]):\n",
    "        temp = test_strokes[i].shape[0]\n",
    "        if temp <= min_stroke:\n",
    "            min_stroke = temp\n",
    "    return min_stroke\n",
    "\n",
    "def filter_stroke(data_stroke, data_label, target_stroke):\n",
    "    target=[]\n",
    "    target_label = []\n",
    "    for i in range(data_stroke.shape[0]):\n",
    "        orig = data_stroke[i].shape[0]\n",
    "        if orig <= target_stroke:\n",
    "            target.append(data_stroke[i])\n",
    "            target_label.append(data_label[i])\n",
    "        else:\n",
    "            continue\n",
    "    return target, target_label\n",
    "\n",
    "def pad_stroke(data_stroke, max_stroke):\n",
    "    for i in range(len(data_stroke)):\n",
    "        orig = data_stroke[i].shape[0]\n",
    "        data_stroke[i] = np.vstack((data_stroke[i], \n",
    "                                    np.hstack((np.zeros((max_stroke-orig,2)),np.ones((max_stroke-orig,1))))))\n",
    "    return data_stroke\n",
    "\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('There are {} ({:.2f} million) parameters in this neural network'.format(\n",
    "        nb_param, nb_param/1e6)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_stroke = 60\n",
    "valid_strokes_ls, valid_label_ls = filter_stroke(valid_strokes,valid_labels,target_stroke)\n",
    "train_strokes_ls, train_label_ls = filter_stroke(train_strokes,train_labels,target_stroke)\n",
    "test_strokes_ls, test_label_ls = filter_stroke(test_strokes,test_labels,target_stroke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_strokes_pad = pad_stroke(valid_strokes_ls,target_stroke)\n",
    "train_strokes_pad = pad_stroke(train_strokes_ls,target_stroke)\n",
    "test_strokes_pad = pad_stroke(test_strokes_ls,target_stroke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_strokes_ar = np.stack(valid_strokes_pad)\n",
    "train_strokes_ar = np.stack(train_strokes_pad)\n",
    "test_strokes_ar = np.stack(test_strokes_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4390, 60, 3)\n"
     ]
    }
   ],
   "source": [
    "print(valid_strokes_ar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_inputs_ts = torch.Tensor(valid_strokes_ar)\n",
    "train_inputs_ts = torch.Tensor(train_strokes_ar)\n",
    "test_inputs_ts = torch.Tensor(test_strokes_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_inputs = torch.cat((valid_inputs_ts, train_inputs_ts, test_inputs_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_label_ts = torch.Tensor(np.array(valid_label_ls))\n",
    "train_label_ts = torch.Tensor(np.array(train_label_ls))\n",
    "test_label_ts = torch.Tensor(np.array(test_label_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_labels = torch.cat((valid_label_ts, train_label_ts, test_label_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 has 4388\n",
      "Label 1 has 10349\n",
      "Label 2 has 34566\n",
      "Label 3 has 52950\n",
      "Label 4 has 31292\n"
     ]
    }
   ],
   "source": [
    "print('Label 0 has', sum(overall_labels.numpy()==0))\n",
    "print('Label 1 has', sum(overall_labels.numpy()==1))\n",
    "print('Label 2 has', sum(overall_labels.numpy()==2))\n",
    "print('Label 3 has', sum(overall_labels.numpy()==3))\n",
    "print('Label 4 has', sum(overall_labels.numpy()==4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test indices\n",
      "10316\n",
      "trainlabel 0 has 4000\n",
      "trainlabel 1 has 4000\n",
      "trainlabel 2 has 4000\n",
      "trainlabel 3 has 4000\n",
      "trainlabel 4 has 4000\n",
      "test indices\n",
      "21292\n",
      "testlabel 0 has 300\n",
      "testlabel 1 has 300\n",
      "testlabel 2 has 300\n",
      "testlabel 3 has 300\n",
      "testlabel 4 has 300\n"
     ]
    }
   ],
   "source": [
    "# Filter Balanced Data for Train \n",
    "from random import shuffle\n",
    "def balance_filter(train_label_ts, SL, EL,train_inputs_ts, name):\n",
    "    label1_indices = np.array(np.where(train_label_ts.numpy() == 0)).reshape(-1)[SL:EL].tolist()\n",
    "    label2_indices = np.array(np.where(train_label_ts.numpy() == 1)).reshape(-1)[SL:EL].tolist()\n",
    "    label3_indices = np.array(np.where(train_label_ts.numpy() == 2)).reshape(-1)[SL:EL].tolist()\n",
    "    label4_indices = np.array(np.where(train_label_ts.numpy() == 3)).reshape(-1)[SL:EL].tolist()\n",
    "    label5_indices = np.array(np.where(train_label_ts.numpy() == 4)).reshape(-1)[SL:EL].tolist()\n",
    "    train_indices = []\n",
    "    temp = [label1_indices, label2_indices, label3_indices, label4_indices, label5_indices]\n",
    "    for i in range(5):\n",
    "        target = temp[i]\n",
    "        for item in range(len(target)):\n",
    "            train_indices.append(target[item])\n",
    "    print('test indices')\n",
    "    shuffle(train_indices)\n",
    "    print(train_indices[0])\n",
    "    train_sub_inputs_ts = train_inputs_ts[train_indices,:,:]\n",
    "    train_sub_label_ts = torch.Tensor(train_label_ts.numpy()[train_indices])\n",
    "\n",
    "    print(name +'label 0 has', sum(train_sub_label_ts.numpy()==0))\n",
    "    print(name +'label 1 has', sum(train_sub_label_ts.numpy()==1))\n",
    "    print(name +'label 2 has', sum(train_sub_label_ts.numpy()==2))\n",
    "    print(name +'label 3 has', sum(train_sub_label_ts.numpy()==3))\n",
    "    print(name + 'label 4 has', sum(train_sub_label_ts.numpy()==4))\n",
    "    return train_sub_inputs_ts, train_sub_label_ts\n",
    "\n",
    "train_sub_inputs_ts, train_sub_label_ts = balance_filter(overall_labels,0,4000, overall_inputs, name='train')\n",
    "test_sub_inputs_ts, test_sub_label_ts = balance_filter(overall_labels, 4000,4300, overall_inputs,'test')\n",
    "#valid_sub_inputs_ts, valid_sub_label_ts = balance_filter(overall_labels, 4000,4300, overall_inputs, 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = train_sub_inputs_ts.numpy(), train_sub_label_ts.numpy()\n",
    "testX, testY = test_sub_inputs_ts.numpy(), test_sub_label_ts.numpy()\n",
    "# valX, valY = valid_sub_inputs_ts.numpy(), valid_sub_label_ts.numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoder\n",
    "trainY_Onehot = np.zeros((trainY.shape[0],5))\n",
    "trainY_Onehot[np.arange(trainY.shape[0]),trainY.astype(int)] = 1 \n",
    "testY_Onehot = np.zeros((testY.shape[0],5))\n",
    "testY_Onehot[np.arange(testY.shape[0]),testY.astype(int)] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1.]\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "IND = 10316\n",
    "print(trainY_Onehot[IND])\n",
    "print(trainY[IND])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KERAS model (explained above)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, GRU, LSTM\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, Reshape\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['LSTMUnits'] = 80\n",
    "args['batch'] = 64\n",
    "args['epochs'] = 10\n",
    "args['dropout'] = 0.2\n",
    "args['len_category'] = 5\n",
    "args['enc_dense'] = 256\n",
    "args['num_filters'] = 1\n",
    "args['kernelS'] = 3\n",
    "args['stride'] = 2\n",
    "args['poolS'] = 2 \n",
    "args['dec_dense'] = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(args['LSTMUnits'],return_sequences=False,input_shape=(60,3)))\n",
    "model.add(Dense(args['enc_dense'], activation='relu'))\n",
    "model.add(Reshape((16,16,1)))\n",
    "model.add(Conv2D(args['num_filters'],args['kernelS'],strides=(args['stride'],args['stride']), activation ='relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(args['poolS'],args['poolS'])))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(args['dec_dense'], activation='relu'))\n",
    "model.add(Dropout(args['dropout']))\n",
    "model.add(Dense(args['len_category'], activation='softmax'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "          optimizer='adam',\n",
    "          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_15 (LSTM)               (None, 80)                26880     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 256)               20736     \n",
      "_________________________________________________________________\n",
      "reshape_9 (Reshape)          (None, 16, 16, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 7, 7, 1)           10        \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 49)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 32)                1600      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 49,391\n",
      "Trainable params: 49,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heylamourding/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 15s 964us/step - loss: 0.1170 - acc: 0.5256 - val_loss: 0.0666 - val_acc: 0.7805\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 13s 810us/step - loss: 0.0602 - acc: 0.8056 - val_loss: 0.0437 - val_acc: 0.8560\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 13s 809us/step - loss: 0.0442 - acc: 0.8589 - val_loss: 0.0375 - val_acc: 0.8775\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 13s 808us/step - loss: 0.0365 - acc: 0.8832 - val_loss: 0.0325 - val_acc: 0.8948\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 13s 811us/step - loss: 0.0316 - acc: 0.9006 - val_loss: 0.0317 - val_acc: 0.8958\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 13s 816us/step - loss: 0.0304 - acc: 0.9022 - val_loss: 0.0308 - val_acc: 0.9040\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 13s 810us/step - loss: 0.0275 - acc: 0.9128 - val_loss: 0.0284 - val_acc: 0.9080\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 13s 826us/step - loss: 0.0267 - acc: 0.9162 - val_loss: 0.0283 - val_acc: 0.9115\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 16s 980us/step - loss: 0.0249 - acc: 0.9217 - val_loss: 0.0282 - val_acc: 0.9083\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 13s 822us/step - loss: 0.0233 - acc: 0.9267 - val_loss: 0.0276 - val_acc: 0.9085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f359c708588>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY_Onehot,\n",
    "          batch_size = args['batch'], nb_epoch= args['epochs'], \n",
    "          verbose=1,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_acc(model, X_test, y_test):\n",
    "    y_predict = model.predict(X_test)\n",
    "    y_predict_label = np.argmax(y_predict,axis = 1)\n",
    "    y_test_label = np.argmax(y_test,axis=1)\n",
    "    print(accuracy_score(y_test_label, y_predict_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9093333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "evaluate_acc(model,testX,testY_Onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
