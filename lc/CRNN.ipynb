{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN(embedding)+RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Liu Chang\n",
    "\n",
    "Description: \n",
    "\n",
    "This notebook will demonstrate CNN+RNN hybrid model performance \n",
    "\n",
    "Date:\n",
    "Week 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OaE0W1YXWdPI",
    "outputId": "c60f7bce-ca0c-4553-e6fb-44463608a4e2"
   },
   "outputs": [],
   "source": [
    "import os.path as path\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data has 5 calsses, overall 633244 items.\n",
    "\n",
    "The 5 classes are:  'calendar', 'snowman', 'penguin', 'blackberry', 'teddy-bear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OaE0W1YXWdPI",
    "outputId": "c60f7bce-ca0c-4553-e6fb-44463608a4e2"
   },
   "outputs": [],
   "source": [
    "data_path='/raid5/liuchang/quick_draw_output'\n",
    "from read_data import get_dataset\n",
    "\n",
    "_,train_X,train_Y,_,test_X,test_Y=get_dataset(data_path,'1102_05b633244')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "JvBU0yFJuNj1",
    "outputId": "74547f75-83a9-4e50-a240-b55d4475ad81"
   },
   "outputs": [],
   "source": [
    "labels_count=len(set(test_Y))\n",
    "print(\"The number of classes=\",labels_count)\n",
    "print(\"The number of items=\",len(train_X)+len(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing Method \n",
    "\n",
    "Most of the data preprocessing is done when generating our dataset.\n",
    "\n",
    "`unpack()` is a data padding method, it is ued on minibatched data.\n",
    "\n",
    "It pads each data item(one paint) so that the size is exactly 30x200 (which is the maximal size in our data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5fnH02c-2YR8"
   },
   "outputs": [],
   "source": [
    "def unpack(x,max_strock,max_len):\n",
    "    x_new=torch.zeros(torch.Size([len(x),max_strock,3,max_len]))\n",
    "    for i,item in enumerate(x):\n",
    "        for j,strock in enumerate(item):\n",
    "            #print(strock)\n",
    "            \n",
    "            #print(len(strock[0]))\n",
    "            strock=torch.Tensor(strock)\n",
    "            #print(x_new[i,j,0,:len(strock[0])].shape)\n",
    "            #print(strock[0].shape)\n",
    "            x_new[i,j,0,:len(strock[0])]=strock[0]\n",
    "            x_new[i,j,1,:len(strock[0])]=strock[1]\n",
    "        x_new[i,0,2,0]=len(item)\n",
    "    return x_new\n",
    "\n",
    "def get_max_len(x,xx):\n",
    "    max_len=0\n",
    "    max_strock=0\n",
    "    for i,item in enumerate(x):\n",
    "        max_strock=max(max_strock,len(item))\n",
    "        #print(max_strock)\n",
    "        for j,strock in enumerate(item):\n",
    "            max_len=max(max_len,len(strock[0])) \n",
    "    for i,item in enumerate(xx):\n",
    "        max_strock=max(max_strock,len(item))\n",
    "        #print(max_strock)\n",
    "        for j,strock in enumerate(item):\n",
    "            max_len=max(max_len,len(strock[0])) \n",
    "    return max_strock,max_len\n",
    "\n",
    "max_strock,max_len=get_max_len(train_X,test_X)\n",
    "print(\"Maximal Size=\",max_strock,\"x\",max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method for obtaining accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(test_X,test_Y):\n",
    "    cur_len=0\n",
    "    acc=0\n",
    "    while cur_len<len(test_X):\n",
    "        model.zero_grad()\n",
    "        minibatch_X=test_X[cur_len:cur_len+batch_size]\n",
    "        minibatch_X=unpack(minibatch_X,max_strock,max_len).cuda()\n",
    "        #print(minibatch_X.shape)\n",
    "        minibatch_Y=test_Y[cur_len:cur_len+batch_size]\n",
    "        minibatch_Y=torch.LongTensor(minibatch_Y).cuda()\n",
    "        y_predict=model(minibatch_X)\n",
    "        #print(y_predict)\n",
    "        y_predict=torch.argmax(y_predict,dim=1)\n",
    "        #print(y_predict.shape)\n",
    "        cur_len+=batch_size\n",
    "        acc+=(y_predict==minibatch_Y).sum().item()\n",
    "    return acc/cur_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Define\n",
    "\n",
    "Here we define out CRNN model.\n",
    "\n",
    "First we define our CNN. it is a 4-layer CNN model as a embedding method, to encode one stroke with size (200,), to a vector with size (64,).\n",
    "\n",
    "During tunning process, I find that \n",
    "1. **The final convolution layer should have a big reception field (kerenel size) to guarantee a better model performance**\n",
    "2. **The linear (dense) vector as a final layer is needed. Ending with convolution layer can not have a good perfomance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5vSbqojtvBOQ"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=3,      # input height\n",
    "                out_channels=6,    # n_filters\n",
    "                kernel_size=3      # filter size\n",
    "            ),\n",
    "            nn.ReLU(),    # activation\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "             nn.Conv1d(6,12, 3),\n",
    "             nn.ReLU(),  # activation\n",
    "             nn.MaxPool1d(kernel_size=2)\n",
    "         )\n",
    "        self.conv3 = nn.Sequential(\n",
    "             nn.Conv1d(12,32, 20),\n",
    "             nn.ReLU(),  # activation\n",
    "             nn.MaxPool1d(kernel_size=10)\n",
    "         )\n",
    "        self.out = nn.Linear(64, output_size)\n",
    "        #self.out2 = nn.Linear(3*(max_len), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x=self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(x.shape)\n",
    "        output = self.out(x)\n",
    "        #output=self.out2(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our CRNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.middle_size=middle_size=40\n",
    "        self.gru=nn.GRU(input_size=middle_size,hidden_size=5,num_layers=3,bias=True)\n",
    "        # self.gru=nn.GRU(input_size=3,hidden_size=5,num_layers=1,bias=True)\n",
    "        self.cnn=CNN(output_size=middle_size)\n",
    "        self.out=nn.Softmax(dim=-1)\n",
    "    def forward(self, x):\n",
    "        x=x.view(-1,3,max_len)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        xx = self.cnn(x)\n",
    "        xx=xx.view(-1,max_strock,self.middle_size).transpose(0,1)\n",
    "        output,_ = self.gru(xx)\n",
    "        #print(output.shape)\n",
    "        to_send=torch.Tensor(torch.Size([xx.shape[1],5])).cuda()\n",
    "        for i in range(xx.shape[1]):\n",
    "            to_send[i]=output[int(x[i*max_strock,2,0])-1,i]\n",
    "        output = self.out(to_send)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model\n",
    "\n",
    "Here we set our batch_size to be very big because each data do not have a big size (just 30x200))\n",
    "\n",
    "We use Checkpoint to save the current best model. Also by using this method we don't have to keep this notebook open to ensure the ouptus been saved.\n",
    "\n",
    "We train our model on NVIDIA Tesla K80 with 11GB memory, training about 12 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model=CRNN().cuda()\n",
    "batch_size=512\n",
    "loss=torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "EPOCH=200\n",
    "CHANGE_METHOD=10\n",
    "bst_acc=0.00\n",
    "for i in range(EPOCH):\n",
    "    total_loss=0\n",
    "    cur_len=0       \n",
    "    while cur_len<len(train_X):\n",
    "        model.zero_grad()\n",
    "        minibatch_X=train_X[cur_len:cur_len+batch_size]\n",
    "        minibatch_X=unpack(minibatch_X,max_strock,max_len).cuda()\n",
    "        \n",
    "        minibatch_Y=train_Y[cur_len:cur_len+batch_size]\n",
    "        minibatch_Y=torch.LongTensor(minibatch_Y).cuda()\n",
    "        y_predict=model(minibatch_X)\n",
    "        \n",
    "        \n",
    "        output_loss=loss(y_predict,minibatch_Y)\n",
    "        output_loss.backward()\n",
    "        optimizer.step()\n",
    "        cur_len+=batch_size\n",
    "        total_loss+=output_loss.item()\n",
    "        #print(\"Processing: {}/{}, loss={}\".format(cur_len,train_X.shape[0],output_loss.item()))\n",
    "    if i%5==0:\n",
    "        acc=get_acc(test_X,test_Y)\n",
    "        print(\"EPOCH {}/{}, loss={},acc={}\".format(i,EPOCH,total_loss,acc))\n",
    "        if acc>bst_acc:\n",
    "            torch.save(model,\"checkpoint.pkl\")\n",
    "            import pickle\n",
    "            pickle.dump((i,acc),open('record','wb'))\n",
    "            bst_acc=acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performace in EPOCH=75 with accuracy=0.9261907762096774\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "bst_epoch,bst_acc=pickle.load(open('record','rb'))\n",
    "print('Best performace in EPOCH={} with accuracy={}'.format(bst_epoch,bst_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CRNN.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
