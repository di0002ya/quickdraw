{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN(embedding)+RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Liu Chang\n",
    "\n",
    "Description: \n",
    "\n",
    "This notebook will demonstrate CNN+RNN hybrid model performance \n",
    "\n",
    "Date:\n",
    "Week 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OaE0W1YXWdPI",
    "outputId": "c60f7bce-ca0c-4553-e6fb-44463608a4e2"
   },
   "outputs": [],
   "source": [
    "import os.path as path\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data has 5 calsses, overall 633244 items.\n",
    "\n",
    "The 5 classes are:  'calendar', 'snowman', 'penguin', 'blackberry', 'teddy-bear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OaE0W1YXWdPI",
    "outputId": "c60f7bce-ca0c-4553-e6fb-44463608a4e2"
   },
   "outputs": [],
   "source": [
    "data_path='/raid5/liuchang/quick_draw_output'\n",
    "from read_data import get_dataset\n",
    "\n",
    "_,train_X,train_Y,_,test_X,test_Y=get_dataset(data_path,'1102_05b633244')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "JvBU0yFJuNj1",
    "outputId": "74547f75-83a9-4e50-a240-b55d4475ad81"
   },
   "outputs": [],
   "source": [
    "labels_count=len(set(test_Y))\n",
    "print(\"The number of classes=\",labels_count)\n",
    "print(\"The number of items=\",len(train_X)+len(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing Method \n",
    "\n",
    "Most of the data preprocessing is done when generating our dataset.\n",
    "\n",
    "`unpack()` is a data padding method, it is ued on minibatched data.\n",
    "\n",
    "It pads each data item(one paint) so that the size is exactly 30x200 (which is the maximal size in our data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5fnH02c-2YR8"
   },
   "outputs": [],
   "source": [
    "def unpack(x,max_strock,max_len):\n",
    "    x_new=torch.zeros(torch.Size([len(x),max_strock,3,max_len]))\n",
    "    for i,item in enumerate(x):\n",
    "        for j,strock in enumerate(item):\n",
    "            strock=torch.Tensor(strock)\n",
    "            x_new[i,j,0,:len(strock[0])]=strock[0]\n",
    "            x_new[i,j,1,:len(strock[0])]=strock[1]\n",
    "        x_new[i,0,2,0]=len(item)\n",
    "    return x_new\n",
    "\n",
    "def get_max_len(x,xx):\n",
    "    max_len=0\n",
    "    max_strock=0\n",
    "    for i,item in enumerate(x):\n",
    "        max_strock=max(max_strock,len(item))\n",
    "        for j,strock in enumerate(item):\n",
    "            max_len=max(max_len,len(strock[0])) \n",
    "    for i,item in enumerate(xx):\n",
    "        max_strock=max(max_strock,len(item))\n",
    "        for j,strock in enumerate(item):\n",
    "            max_len=max(max_len,len(strock[0])) \n",
    "    return max_strock,max_len\n",
    "\n",
    "max_strock,max_len=get_max_len(train_X,test_X)\n",
    "print(\"Maximal Size=\",max_strock,\"x\",max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method for obtaining accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(test_X,test_Y):\n",
    "    cur_len=0\n",
    "    acc=0\n",
    "    while cur_len<len(test_X):\n",
    "        model.zero_grad()\n",
    "        minibatch_X=test_X[cur_len:cur_len+batch_size]\n",
    "        minibatch_X=unpack(minibatch_X,max_strock,max_len).cuda()\n",
    "        minibatch_Y=test_Y[cur_len:cur_len+batch_size]\n",
    "        minibatch_Y=torch.LongTensor(minibatch_Y).cuda()\n",
    "        y_predict=model(minibatch_X)\n",
    "        y_predict=torch.argmax(y_predict,dim=1)\n",
    "        cur_len+=batch_size\n",
    "        acc+=(y_predict==minibatch_Y).sum().item()\n",
    "    return acc/cur_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Define\n",
    "\n",
    "Here we define out CRNN model.\n",
    "\n",
    "First we define our CNN. it is a 4-layer CNN model as a embedding method, to encode one stroke with size (200,), to a vector with size (64,).\n",
    "\n",
    "During tunning process, I find that \n",
    "1. **The final convolution layer should have a big reception field (kerenel size) to guarantee a better model performance**\n",
    "2. **The linear (dense) vector as a final layer is needed. Ending with convolution layer can not have a good perfomance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5vSbqojtvBOQ"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=3,      # input height\n",
    "                out_channels=6,    # n_filters\n",
    "                kernel_size=3      # filter size\n",
    "            ),\n",
    "            nn.ReLU(),    # activation\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "             nn.Conv1d(6,12, 3),\n",
    "             nn.ReLU(),  # activation\n",
    "             nn.MaxPool1d(kernel_size=2)\n",
    "         )\n",
    "        self.conv3 = nn.Sequential(\n",
    "             nn.Conv1d(12,32, 20),\n",
    "             nn.ReLU(),  # activation\n",
    "             nn.MaxPool1d(kernel_size=10)\n",
    "         )\n",
    "        self.out = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x=self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our CRNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.middle_size=middle_size=40\n",
    "        self.gru=nn.GRU(input_size=middle_size,hidden_size=5,num_layers=3,bias=True)\n",
    "        # self.gru=nn.GRU(input_size=3,hidden_size=5,num_layers=1,bias=True)\n",
    "        self.cnn=CNN(output_size=middle_size)\n",
    "        self.out=nn.Softmax(dim=-1)\n",
    "    def forward(self, x):\n",
    "        x=x.view(-1,3,max_len)\n",
    "        xx = self.cnn(x)\n",
    "        xx=xx.view(-1,max_strock,self.middle_size).transpose(0,1)\n",
    "        output,_ = self.gru(xx)\n",
    "        to_send=torch.Tensor(torch.Size([xx.shape[1],5])).cuda()\n",
    "        for i in range(xx.shape[1]):\n",
    "            to_send[i]=output[int(x[i*max_strock,2,0])-1,i]\n",
    "        output = self.out(to_send)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model\n",
    "\n",
    "Here we set our batch_size to be very big because each data do not have a big size (just 30x200))\n",
    "\n",
    "We use Checkpoint to save the current best model. Also by using this method we don't have to keep this notebook open to ensure the ouptus been saved.\n",
    "\n",
    "We train our model on NVIDIA Tesla K80 with 11GB memory, training about 12 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0/200, loss=1392.487428188324,acc=0.7514569682459677\n",
      " 1/200, loss=1278.0826328992844,acc=0.8673056325604839\n",
      " 2/200, loss=1254.408109664917,acc=0.8779926915322581\n",
      " 3/200, loss=1245.3379324674606,acc=0.8845766129032258\n",
      " 4/200, loss=1240.8834501504898,acc=0.8928065146169355\n",
      " 5/200, loss=1237.5869688987732,acc=0.898894279233871\n",
      " 6/200, loss=1234.294924736023,acc=0.8970435357862904\n",
      " 7/200, loss=1231.2056339979172,acc=0.9066437752016129\n",
      " 8/200, loss=1229.548694729805,acc=0.9063208795362904\n",
      " 9/200, loss=1228.1950545310974,acc=0.912077872983871\n",
      " 10/200, loss=1226.7557901144028,acc=0.9128890498991935\n",
      " 11/200, loss=1226.493235707283,acc=0.9126449092741935\n",
      " 12/200, loss=1223.6590828895569,acc=0.9166456653225806\n",
      " 13/200, loss=1223.37473154068,acc=0.9122275075604839\n",
      " 14/200, loss=1222.13602745533,acc=0.9184412802419355\n",
      " 15/200, loss=1221.5133837461472,acc=0.9183940272177419\n",
      " 16/200, loss=1221.1248507499695,acc=0.9180553805443549\n",
      " 17/200, loss=1220.1645115613937,acc=0.9139522429435484\n",
      " 18/200, loss=1220.2491985559464,acc=0.9188508064516129\n",
      " 19/200, loss=1219.611900806427,acc=0.9233162172379032\n",
      " 20/200, loss=1219.134142756462,acc=0.9217017389112904\n",
      " 21/200, loss=1218.7196065187454,acc=0.9237178679435484\n",
      " 22/200, loss=1218.071389913559,acc=0.9220325100806451\n",
      " 23/200, loss=1217.9947981834412,acc=0.9217174899193549\n",
      " 24/200, loss=1217.1542348861694,acc=0.9251118321572581\n",
      " 25/200, loss=1217.0000692605972,acc=0.9251827116935484\n",
      " 26/200, loss=1216.3597564697266,acc=0.925568611391129\n",
      " 27/200, loss=1216.101459145546,acc=0.9244424143145161\n",
      " 28/200, loss=1215.7512040138245,acc=0.9269547001008065\n",
      " 29/200, loss=1215.8470075130463,acc=0.9254977318548387\n",
      " 30/200, loss=1215.8472524881363,acc=0.9271043346774194\n",
      " 31/200, loss=1215.0688072443008,acc=0.9265924269153226\n",
      " 32/200, loss=1214.984782576561,acc=0.9292149697580645\n",
      " 33/200, loss=1214.8148691654205,acc=0.9280493951612904\n",
      " 34/200, loss=1214.2350809574127,acc=0.9250882056451613\n",
      " 35/200, loss=1214.7119799852371,acc=0.928254158266129\n",
      " 36/200, loss=1213.9415044784546,acc=0.9307585685483871\n",
      " 37/200, loss=1213.9225682020187,acc=0.9284904233870968\n",
      " 38/200, loss=1213.6987439393997,acc=0.9310027091733871\n",
      " 39/200, loss=1213.541815161705,acc=0.9305459299395161\n",
      " 40/200, loss=1213.5119576454163,acc=0.9309869581653226\n",
      " 41/200, loss=1213.3381180763245,acc=0.9295693674395161\n",
      " 42/200, loss=1212.942211985588,acc=0.9307979460685484\n",
      " 43/200, loss=1212.800575017929,acc=0.9291440902217742\n",
      " 44/200, loss=1212.954875946045,acc=0.929719002016129\n",
      " 45/200, loss=1212.7690362930298,acc=0.9293882308467742\n",
      " 46/200, loss=1212.8233217000961,acc=0.9175513482862904\n",
      " 47/200, loss=1212.7183456420898,acc=0.9290889616935484\n",
      " 48/200, loss=1212.3183192014694,acc=0.9311680947580645\n",
      " 49/200, loss=1212.1191757917404,acc=0.9315697454637096\n",
      " 50/200, loss=1212.0267318487167,acc=0.9305853074596774\n",
      " 51/200, loss=1211.670536518097,acc=0.9298686365927419\n",
      " 52/200, loss=1212.0649206638336,acc=0.9323572958669355\n",
      " 53/200, loss=1211.7835958003998,acc=0.9324990549395161\n",
      " 54/200, loss=1212.3282824754715,acc=0.9315303679435484\n",
      " 55/200, loss=1211.8343183994293,acc=0.9302466607862904\n",
      " 56/200, loss=1211.4643796682358,acc=0.9327589465725806\n",
      " 57/200, loss=1211.5231283903122,acc=0.9300891507056451\n",
      " 58/200, loss=1211.1720967292786,acc=0.9314516129032258\n",
      " 59/200, loss=1211.3870757818222,acc=0.9297111265120968\n",
      " 60/200, loss=1211.1337169408798,acc=0.932097404233871\n",
      " 61/200, loss=1210.9887608289719,acc=0.9320344002016129\n",
      " 62/200, loss=1211.0743516683578,acc=0.9336646295362904\n",
      " 63/200, loss=1210.817172050476,acc=0.9313886088709677\n",
      " 64/200, loss=1211.012285232544,acc=0.9316485005040323\n",
      " 65/200, loss=1210.5921469926834,acc=0.9351452242943549\n",
      " 66/200, loss=1210.8041211366653,acc=0.9352397303427419\n",
      " 67/200, loss=1210.4089591503143,acc=0.9324596774193549\n",
      " 68/200, loss=1210.6448982954025,acc=0.9310263356854839\n",
      " 69/200, loss=1210.5761560201645,acc=0.9321919102822581\n",
      " 70/200, loss=1210.4545764923096,acc=0.9361217867943549\n",
      " 71/200, loss=1210.6172457933426,acc=0.9344285534274194\n",
      " 72/200, loss=1210.233751296997,acc=0.9348774571572581\n",
      " 73/200, loss=1210.244084239006,acc=0.934326171875\n",
      " 74/200, loss=1210.2449992895126,acc=0.9343419228830645\n",
      " 75/200, loss=1210.6349470615387,acc=0.9349562121975806\n",
      " 76/200, loss=1209.9905529022217,acc=0.9328455771169355\n",
      " 77/200, loss=1210.7921245098114,acc=0.9331527217741935\n",
      " 78/200, loss=1210.1185213327408,acc=0.9356098790322581\n",
      " 79/200, loss=1209.886090874672,acc=0.9367045740927419\n",
      " 80/200, loss=1209.8693338632584,acc=0.9333889868951613\n",
      " 81/200, loss=1209.9131008386612,acc=0.9325226814516129\n",
      " 82/200, loss=1209.7906602621078,acc=0.9330188382056451\n",
      " 83/200, loss=1209.8664736747742,acc=0.9349719632056451\n",
      " 84/200, loss=1209.9966510534286,acc=0.9315303679435484\n",
      " 85/200, loss=1209.632841348648,acc=0.9344915574596774\n",
      " 86/200, loss=1209.747938632965,acc=0.9314988659274194\n",
      " 87/200, loss=1210.2502448558807,acc=0.9355862525201613\n",
      " 88/200, loss=1209.6157879829407,acc=0.9308924521169355\n",
      " 89/200, loss=1209.6875007152557,acc=0.9355547505040323\n",
      " 90/200, loss=1209.6584337949753,acc=0.9320659022177419\n",
      " 91/200, loss=1209.5394316911697,acc=0.9296953755040323\n",
      " 92/200, loss=1209.7875417470932,acc=0.9303175403225806\n",
      " 93/200, loss=1209.5250470638275,acc=0.9328692036290323\n",
      " 94/200, loss=1209.385379076004,acc=0.9330030871975806\n",
      " 95/200, loss=1209.3883934020996,acc=0.9333574848790323\n",
      " 96/200, loss=1209.3240243196487,acc=0.9319005166330645\n",
      " 97/200, loss=1209.2939636707306,acc=0.9376575100806451\n",
      " 98/200, loss=1209.3683087825775,acc=0.9358933971774194\n",
      " 99/200, loss=1209.411662220955,acc=0.9329637096774194\n",
      " 100/200, loss=1209.2922772169113,acc=0.9339717741935484\n",
      " 101/200, loss=1209.2875500917435,acc=0.93359375\n",
      " 102/200, loss=1208.9597189426422,acc=0.9332866053427419\n",
      " 103/200, loss=1209.165631890297,acc=0.9348302041330645\n",
      " 104/200, loss=1209.1772409677505,acc=0.9366966985887096\n",
      " 105/200, loss=1209.7605485916138,acc=0.9321525327620968\n",
      " 106/200, loss=1209.0883672237396,acc=0.9337276335685484\n",
      " 107/200, loss=1209.1411360502243,acc=0.934357673891129\n",
      " 108/200, loss=1209.1669247150421,acc=0.9351530997983871\n",
      " 109/200, loss=1208.9368215799332,acc=0.9369250882056451\n",
      " 110/200, loss=1208.9852010011673,acc=0.9346096900201613\n",
      " 111/200, loss=1208.9527047872543,acc=0.936799080141129\n",
      " 112/200, loss=1208.861983180046,acc=0.9354287424395161\n",
      " 113/200, loss=1208.9679886102676,acc=0.9369250882056451\n",
      " 114/200, loss=1208.9162244796753,acc=0.9325541834677419\n",
      " 115/200, loss=1209.029779434204,acc=0.9347514490927419\n",
      " 116/200, loss=1209.0021917819977,acc=0.9342789188508065\n",
      " 117/200, loss=1208.9632821083069,acc=0.9354523689516129\n",
      " 118/200, loss=1208.7647223472595,acc=0.9335858744959677\n",
      " 119/200, loss=1208.9359550476074,acc=0.9371613533266129\n",
      " 120/200, loss=1208.8783172369003,acc=0.9354444934475806\n",
      " 121/200, loss=1208.8682695627213,acc=0.9363501764112904\n",
      " 122/200, loss=1208.8922477960587,acc=0.9357358870967742\n",
      " 123/200, loss=1208.8230508565903,acc=0.9301206527217742\n",
      " 124/200, loss=1208.4285353422165,acc=0.9346096900201613\n",
      " 125/200, loss=1208.6729575395584,acc=0.9335307459677419\n",
      " 126/200, loss=1208.9020700454712,acc=0.9374291204637096\n",
      " 127/200, loss=1208.3318150043488,acc=0.9350349672379032\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import pickle\n",
    "print(0)\n",
    "print(1)\n",
    "model=CRNN().cuda()\n",
    "batch_size=512\n",
    "loss=torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "EPOCH=200\n",
    "\n",
    "bst_acc=0.00\n",
    "for i in range(EPOCH):\n",
    "    total_loss=0\n",
    "    cur_len=0       \n",
    "    while cur_len<len(train_X):\n",
    "        model.zero_grad()\n",
    "        minibatch_X=train_X[cur_len:cur_len+batch_size]\n",
    "        minibatch_X=unpack(minibatch_X,max_strock,max_len).cuda()\n",
    "        \n",
    "        minibatch_Y=train_Y[cur_len:cur_len+batch_size]\n",
    "        minibatch_Y=torch.LongTensor(minibatch_Y).cuda()\n",
    "        y_predict=model(minibatch_X)\n",
    "        \n",
    "        \n",
    "        output_loss=loss(y_predict,minibatch_Y)\n",
    "        output_loss.backward()\n",
    "        optimizer.step()\n",
    "        cur_len+=batch_size\n",
    "        total_loss+=output_loss.item()\n",
    "    if i%1==0:\n",
    "        acc=get_acc(test_X,test_Y)\n",
    "        record_file=open('record','a')\n",
    "        record_file.write(\"{}/{}, loss={},acc={}\".format(i,EPOCH,total_loss,acc))\n",
    "        record_file.close()\n",
    "        if acc>bst_acc:\n",
    "            torch.save(model,\"checkpoint.pkl\")\n",
    "            bst_acc=acc"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CRNN.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
