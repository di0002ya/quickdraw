{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_cnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "zkaNR3VhBoPw",
        "o02U9T9NDWcE"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "8GrIEHEhAKcZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Author: Qiu Yang\n",
        "\n",
        "LSTM encoder, CNN decoder\n",
        "\n",
        "**Settings used for current result:**\n",
        "\n",
        "toy dataset, 5000 training , 1000 testing, [X, Y, 0/1], model_1"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MtsFQfHzulET",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import os.path as path\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.path import Path\n",
        "#import generate_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZmsvnbXRulEZ",
        "outputId": "2dbbd010-ded2-4cbb-f274-f303fff94853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "device= torch.device(\"cuda\")\n",
        "#device= torch.device(\"cpu\")\n",
        "print(device)\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "id": "suS_js-gABnF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Datasets"
      ]
    },
    {
      "metadata": {
        "id": "ZqIfo7oPm20Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**2 class dataset (airplane, aircraft carrier, JP)**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jtkWxu9lulEh",
        "outputId": "a7a525bb-2476-4947-bca5-53b94ae5246b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "data_path  = ''\n",
        "with open(path.join(data_path,'data_X'),'rb') as f:\n",
        "    X=pickle.load(f)\n",
        "    \n",
        "with open(path.join(data_path,'data_Y'),'rb') as f:\n",
        "    Y=pickle.load(f)\n",
        "    \n",
        "len_train_X=int(len(X)*0.8)\n",
        "\n",
        "train_data_raw=np.array(X[:len_train_X])\n",
        "train_label=np.array(Y[:len_train_X])\n",
        "test_data_raw=np.array(X[len_train_X:])\n",
        "test_label=np.array(Y[len_train_X:])\n",
        "\n",
        "\n",
        "print(len(train_data_raw),len(train_label))\n",
        "\n",
        "print(len(test_data_raw),len(test_label))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "93301 93301\n",
            "23326 23326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VbAsJYJynAg8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**4 class dataset (toy dataset)**"
      ]
    },
    {
      "metadata": {
        "id": "Fed10p91nD5Y",
        "colab_type": "code",
        "outputId": "ee3570b7-bcf9-4e86-dcfd-71f9911135ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "data_path  = ''\n",
        "with open(path.join(data_path,'train_X'),'rb') as f:\n",
        "    X=pickle.load(f)\n",
        "    \n",
        "with open(path.join(data_path,'train_Y'),'rb') as f:\n",
        "    Y=pickle.load(f)\n",
        "    \n",
        "len_train_X=int(len(X)*0.8)\n",
        "\n",
        "train_data_raw=np.array(X[:len_train_X])\n",
        "train_label=np.array(Y[:len_train_X])\n",
        "test_data_raw=np.array(X[len_train_X:])\n",
        "test_label=np.array(Y[len_train_X:])\n",
        "\n",
        "\n",
        "print(len(train_data_raw),len(train_label))\n",
        "\n",
        "print(len(test_data_raw),len(test_label))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21406 21406\n",
            "5352 5352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kQq4PTdismBO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**A random sample**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1QvGLEOiulEn",
        "outputId": "f06906e7-b533-4c88-8620-3959a03e2032",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "sketch = random.choice(train_data_raw)\n",
        "for i in range(len(sketch)):\n",
        "    plt.plot(sketch[i][0][:], sketch[i][1][:])\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFKCAYAAADMuCxnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEE5AClkIIYRwAlLIQgghhBP4fzp6eLcHYPJIAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f61d820c908>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "FJEk8gg3ASTG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Sub-sampling(for faster run) and reformat**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "AsYZrNVwulEt",
        "outputId": "08f03765-b2d0-409e-c8ec-5aad878ce5da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "#sub sample for experiment\n",
        "train_size=5000\n",
        "test_size=1000\n",
        "\n",
        "train_sample = train_data_raw[0:train_size]\n",
        "train_label_sample = train_label[0:train_size]\n",
        "\n",
        "test_sample = test_data_raw[0:test_size]\n",
        "test_label_sample = test_label[0:test_size]\n",
        "#train_sample = random.sample(list(train_data_raw),train_size)\n",
        "#test_sample = random.sample(list(test_data_raw),test_size)\n",
        "print(len(train_sample))\n",
        "\n",
        "#reformat into [x,y,l]\n",
        "no_point = 0;\n",
        "#ave_stroke_length = 0 \n",
        "\n",
        "def get_max_len(sub_sample):\n",
        "    max_stroke_length = 0\n",
        "    max_point_length = 0\n",
        "    for i in range(len(sub_sample)): #each image\n",
        "        if len(sub_sample[i])> max_stroke_length: \n",
        "            max_stroke_length = len(sub_sample[i])\n",
        "        for j in range(len(sub_sample[i])): #each strokes\n",
        "            if len(sub_sample[i][j][0])> max_point_length:\n",
        "                max_point_length = len(sub_sample[i][j][0])\n",
        "    return max_stroke_length,max_point_length\n",
        "                \n",
        "max_stroke_len,max_point_len = get_max_len(train_sample)    \n",
        "print (max_stroke_len,max_point_len)\n",
        "                \n",
        "def process_data(sub_sample, append):\n",
        "    stroke_data = []\n",
        "    point_data = []\n",
        "    for i in range(len(sub_sample)): #image\n",
        "        for j in range(len(sub_sample[i])): #strokes\n",
        "            for k in range(len(sub_sample[i][j][0])): #points\n",
        "                if append:\n",
        "                    if k == len(sub_sample[i][j][0])-1:\n",
        "                        temp2 = [sub_sample[i][j][0][k],sub_sample[i][j][1][k],1]\n",
        "                    else:\n",
        "                        temp2 = [sub_sample[i][j][0][k],sub_sample[i][j][1][k],0]\n",
        "                    point_data.append(temp2)\n",
        "                else:\n",
        "                    temp = [sub_sample[i][j][0][k],sub_sample[i][j][1][k]]\n",
        "                    point_data.append(temp)\n",
        "        stroke_data.append(point_data)\n",
        "        point_data = []\n",
        "            \n",
        "    return stroke_data   \n",
        "\n",
        "train_data = process_data(train_sample, 1)\n",
        "test_data = process_data(test_sample, 1)\n",
        "\n",
        "\n",
        "print (len(train_data))\n",
        "print (len(test_data))\n",
        "\n",
        "#print(no_point)\n",
        "#train_data = train_data.view()\n",
        "\n",
        "print(train_data_raw[0])\n",
        "print(train_data[0])\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000\n",
            "59 533\n",
            "5000\n",
            "1000\n",
            "[[[90, 92, 101, 115, 126, 129, 130, 126, 119], [62, 88, 139, 183, 206, 202, 176, 113, 66]], [[36, 18, 1, 9, 32, 48], [64, 110, 187, 180, 134, 82]], [[35, 30, 34, 42, 49, 60, 72, 78, 91, 96], [138, 147, 171, 183, 187, 188, 181, 174, 148, 128]], [[25, 26, 22, 5, 5, 26, 18, 21, 50, 66, 86, 98, 105, 102], [149, 214, 225, 234, 239, 240, 252, 254, 255, 247, 221, 181, 144, 140]], [[105, 109, 104, 92, 72, 32, 18, 0, 24, 34, 39, 38], [64, 25, 14, 4, 0, 11, 20, 25, 28, 34, 47, 64]], [[61, 61], [20, 20]]]\n",
            "[[90, 62, 0], [92, 88, 0], [101, 139, 0], [115, 183, 0], [126, 206, 0], [129, 202, 0], [130, 176, 0], [126, 113, 0], [119, 66, 1], [36, 64, 0], [18, 110, 0], [1, 187, 0], [9, 180, 0], [32, 134, 0], [48, 82, 1], [35, 138, 0], [30, 147, 0], [34, 171, 0], [42, 183, 0], [49, 187, 0], [60, 188, 0], [72, 181, 0], [78, 174, 0], [91, 148, 0], [96, 128, 1], [25, 149, 0], [26, 214, 0], [22, 225, 0], [5, 234, 0], [5, 239, 0], [26, 240, 0], [18, 252, 0], [21, 254, 0], [50, 255, 0], [66, 247, 0], [86, 221, 0], [98, 181, 0], [105, 144, 0], [102, 140, 1], [105, 64, 0], [109, 25, 0], [104, 14, 0], [92, 4, 0], [72, 0, 0], [32, 11, 0], [18, 20, 0], [0, 25, 0], [24, 28, 0], [34, 34, 0], [39, 47, 0], [38, 64, 1], [61, 20, 0], [61, 20, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6t9V5ZoNWOZd",
        "outputId": "9cc10402-6821-43af-9ef1-651722aa7506",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#string to int label\n",
        "label_set = set(train_label_sample)\n",
        "label_list = list(label_set)\n",
        "train_label_tensor = []\n",
        "for i in range(len(train_label_sample)):\n",
        "    train_label_tensor.append(label_list.index(train_label_sample[i]))\n",
        "test_label_set = set(test_label)\n",
        "test_label_list = list(test_label_set)\n",
        "test_label_tensor = []\n",
        "for i in range(len(test_label_sample)):\n",
        "    test_label_tensor.append(test_label_list.index(test_label_sample[i]))  \n",
        "print (len(train_label_tensor),train_label_tensor[0],len(test_label_tensor))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000 3 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nwdeNv6U_L-H",
        "colab_type": "code",
        "outputId": "caf4d7f3-993c-4df5-800a-9d689d55585c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#append labels to match no.points\n",
        "total_train_label = []\n",
        "total_test_label = []\n",
        "\n",
        "for i in range(len(train_data)):\n",
        "    for j in range(len(train_data[i])):\n",
        "        total_train_label.append(train_label_tensor[i])\n",
        "        \n",
        "for i in range(len(test_data)):\n",
        "    for j in range(len(test_data[i])):\n",
        "        total_test_label.append(test_label_tensor[i])\n",
        "        \n",
        "print(len(total_train_label),total_train_label[0])\n",
        "print(len(total_test_label))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "330386 3\n",
            "64036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3CvVRG6i_L-L",
        "colab_type": "code",
        "outputId": "d92db69d-ac26-40de-fbed-7073baaee98c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#a list of all points\n",
        "train_data_points =[]\n",
        "test_data_points = []\n",
        "\n",
        "for i in range(len(train_data)):\n",
        "    train_data_points.extend(train_data[i])\n",
        "    \n",
        "for i in range(len(test_data)):\n",
        "    test_data_points.extend(test_data[i])\n",
        "    \n",
        "print (len(train_data_points),train_data_points[0])\n",
        "print (len(test_data_points))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "330386 [90, 62, 0]\n",
            "64036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t5AZJyDtA7UZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-on5907pulEw",
        "outputId": "171b68d2-9c1b-4151-ec70-0a052081cf25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#parameters\n",
        "#train\n",
        "bs = 50\n",
        "lr = 12\n",
        "Epoch = 12\n",
        "#lstm\n",
        "input_size = 3\n",
        "hidden_size = 300\n",
        "feature_size = 64\n",
        "seq_len = 1#max_stroke_length\n",
        "#cnn\n",
        "channels = 1 #h_seq, h_final, c_final\n",
        "kernels = 50\n",
        "kernel_size = 5\n",
        "pool_size = 2\n",
        "size_after_pool = int(bs*seq_len*hidden_size*channels*kernels/(pool_size*pool_size))\n",
        "output_size = 4\n",
        "temp_size = 10000\n",
        "\n",
        "print(size_after_pool)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "187500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hP03VKgjX7d5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Model_1: one score for each point**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4nOUBWUOulEz",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class lstm_cnn (nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(lstm_cnn, self).__init__()\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "\n",
        "        self.conv = nn.Conv2d(channels, kernels, kernel_size = kernel_size, padding = 1)\n",
        "        self.pool = nn.MaxPool2d(pool_size, pool_size)\n",
        "        #self.linear1 = nn.Linear(2500 , 2000)\n",
        "        self.linear2 = nn.Linear(2700, 100)\n",
        "        self.output = nn.Linear(100, output_size) \n",
        "\n",
        "        \n",
        "    def forward(self, point_seq, h_init, c_init ):\n",
        "        score_seq = torch.LongTensor([0])\n",
        "        h_seq , (h_final,c_final)  =   self.lstm( point_seq , (h_init,c_init) )\n",
        "        h_seq_shaped = h_seq.view(1,1,-1,hidden_size)\n",
        "        #print('shape',h_seq_shaped.shape)\n",
        "        for i in range(h_seq_shaped.shape[2]):\n",
        "            temp = h_seq_shaped[:,:,i,:]\n",
        "            temp = temp.reshape(1,1,20,15)\n",
        "            temp = self.conv(temp)\n",
        "            temp = F.relu(temp)\n",
        "            temp = self.pool(temp)\n",
        "\n",
        "            temp_size = torch.numel(temp)\n",
        "            temp = temp.reshape(-1,temp_size)\n",
        "            #temp = self.linear1(temp)\n",
        "            #temp = F.relu(temp)\n",
        "\n",
        "            temp = self.linear2(temp)\n",
        "            temp = F.relu(temp)\n",
        "            score = self.output(temp)\n",
        "            \n",
        "            if i== 0:\n",
        "                score_seq = score\n",
        "                #print(score_seq.shape, score_seq)\n",
        "            else:\n",
        "                score_seq = torch.cat((score_seq, score),0)\n",
        "            #print('score_seq shape', score_seq.shape)\n",
        "        return score_seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qAwd4QPWbj50",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**(WIP)  Model_2: one score fore each stroke**"
      ]
    },
    {
      "metadata": {
        "id": "oHA03dprbkEv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class lstm_cnn (nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(lstm_cnn, self).__init__()\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "\n",
        "        self.conv = nn.Conv2d(channels, kernels, kernel_size = kernel_size, padding = 1)\n",
        "        self.pool = nn.MaxPool2d(pool_size, pool_size)\n",
        "        #self.linear1 = nn.Linear(2500 , 2000)\n",
        "        self.linear2 = nn.Linear(1800, 100)\n",
        "        self.output = nn.Linear(100, output_size) \n",
        "\n",
        "        \n",
        "    def forward(self, input_data, h_init, c_init ):\n",
        "        score_seq = torch.LongTensor([0])\n",
        "        h_seq , (h_final,c_final)  =   self.lstm( input_data , (h_init,c_init) )\n",
        "        #h_seq_shaped = h_seq.view(bs,channels,-1,hidden_size)\n",
        "        print('shape',h_seq.shape,'final',h_final.shape)\n",
        "        temp = h_seq_shaped[:,:,i,:]\n",
        "        temp = temp.reshape(1,1,20,10)\n",
        "        temp = self.conv(temp)\n",
        "        temp = F.relu(temp)\n",
        "        temp = self.pool(temp)\n",
        "\n",
        "        temp_size = torch.numel(temp)\n",
        "        temp = temp.reshape(-1,temp_size)\n",
        "        #temp = self.linear1(temp)\n",
        "        #temp = F.relu(temp)\n",
        "\n",
        "        temp = self.linear2(temp)\n",
        "        temp = F.relu(temp)\n",
        "        score = self.output(temp)\n",
        "\n",
        "        if i== 0:\n",
        "            score_seq = score\n",
        "            #print(score_seq.shape, score_seq)\n",
        "        else:\n",
        "            score_seq = torch.cat((score_seq, score),0)\n",
        "        #print('score_seq shape', score_seq.shape)\n",
        "        return score_seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3rxaHZ9fhNZw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Build the net**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Jio9zcAJulE1",
        "outputId": "5b327d65-64f8-4b3b-94da-d58f89094866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "def display_num_param(net):\n",
        "    nb_param = 0\n",
        "    for param in net.parameters():\n",
        "        nb_param += param.numel()\n",
        "    print('There are {} ({:.2f} million) parameters in this neural network'.format(\n",
        "        nb_param, nb_param/1e6)\n",
        "         )\n",
        "\n",
        "net = lstm_cnn()\n",
        "\n",
        "print(net)\n",
        "\n",
        "display_num_param(net)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lstm_cnn(\n",
            "  (lstm): LSTM(3, 300, batch_first=True)\n",
            "  (conv): Conv2d(1, 50, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (linear2): Linear(in_features=2700, out_features=100, bias=True)\n",
            "  (output): Linear(in_features=100, out_features=4, bias=True)\n",
            ")\n",
            "There are 637804 (0.64 million) parameters in this neural network\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Lt68k2gSulE5",
        "outputId": "234f2f3a-fe75-4f88-a5bf-3d9c6d008e31",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "cell_type": "code",
      "source": [
        "net = net.to(device)\n",
        "\n",
        "#net.linear.weight.data.uniform_(-0.1, 0.1)\n",
        "#net.linear1.weight.data.uniform_(-0.1, 0.1)\n",
        "net.linear2.weight.data.uniform_(-0.1, 0.1)\n",
        "net.output.weight.data.uniform_(-0.1, 0.1)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0353, -0.0470, -0.0785,  0.0473, -0.0040,  0.0640,  0.0076, -0.0365,\n",
              "          0.0429,  0.0730,  0.0597,  0.0151, -0.0443,  0.0789,  0.0249,  0.0795,\n",
              "          0.0330,  0.0012,  0.0249, -0.0459, -0.0376, -0.0431, -0.0623,  0.0117,\n",
              "          0.0431, -0.0301,  0.0732, -0.0116, -0.0264,  0.0811,  0.0859,  0.0032,\n",
              "         -0.0932, -0.0891, -0.0448,  0.0915, -0.0935, -0.0764,  0.0032,  0.0841,\n",
              "         -0.0569,  0.0660, -0.0243,  0.0237,  0.0513,  0.0632,  0.0810, -0.0894,\n",
              "         -0.0856, -0.0665,  0.0193, -0.0638, -0.0359, -0.0533, -0.0244,  0.0364,\n",
              "         -0.0641, -0.0658, -0.0245,  0.0349, -0.0492, -0.0542,  0.0700,  0.0056,\n",
              "         -0.0478, -0.0275,  0.0596, -0.0773,  0.0886,  0.0449,  0.0631, -0.0349,\n",
              "         -0.0760, -0.0831,  0.0001, -0.0336, -0.0146,  0.0447,  0.0302,  0.0961,\n",
              "          0.0772,  0.0544,  0.0640,  0.0644, -0.0059, -0.0494, -0.0705, -0.0722,\n",
              "          0.0990,  0.0851, -0.0642,  0.0249, -0.0971, -0.0014,  0.0792, -0.0372,\n",
              "         -0.0558,  0.0056, -0.0886, -0.0775],\n",
              "        [-0.0217,  0.0633,  0.0212,  0.0756, -0.0347,  0.0787, -0.0164, -0.0652,\n",
              "         -0.0131,  0.0372, -0.0562,  0.0435, -0.0075, -0.0876, -0.0686, -0.0339,\n",
              "          0.0348,  0.0279, -0.0775, -0.0284,  0.0852, -0.0753, -0.0654,  0.0649,\n",
              "          0.0269,  0.0441, -0.0272,  0.0972, -0.0688, -0.0793,  0.0096,  0.0935,\n",
              "         -0.0260, -0.0582, -0.0300, -0.0933,  0.0696, -0.0853, -0.0658,  0.0909,\n",
              "         -0.0994,  0.0018, -0.0159,  0.0796, -0.0909,  0.0408, -0.0338, -0.0093,\n",
              "         -0.0126, -0.0591, -0.0352, -0.0652, -0.0546,  0.0282, -0.0874, -0.0566,\n",
              "         -0.0688,  0.0301,  0.0908,  0.0273, -0.0418, -0.0705,  0.0441,  0.0190,\n",
              "          0.0755, -0.0280,  0.0922, -0.0072,  0.0124, -0.0390, -0.0967,  0.0522,\n",
              "         -0.0522, -0.0505, -0.0362,  0.0964,  0.0084,  0.0394, -0.0444,  0.0094,\n",
              "         -0.0357,  0.0327,  0.0566,  0.0670, -0.0491,  0.0512, -0.0132,  0.0394,\n",
              "         -0.0077,  0.0229, -0.0523,  0.0209, -0.0892, -0.0228, -0.0012,  0.0382,\n",
              "          0.0269,  0.0161, -0.0662,  0.0379],\n",
              "        [-0.0078, -0.0693, -0.0121,  0.0765, -0.0671, -0.0035, -0.0772,  0.0578,\n",
              "         -0.0009, -0.0857,  0.0273,  0.0517,  0.0921, -0.0586, -0.0860, -0.0036,\n",
              "         -0.0885,  0.0689,  0.0482,  0.0563, -0.0703, -0.0004, -0.0694,  0.0801,\n",
              "         -0.0153,  0.0475,  0.0477, -0.0386,  0.0832, -0.0693,  0.0974, -0.0264,\n",
              "         -0.0169,  0.0492, -0.0386,  0.0599, -0.0279,  0.0274, -0.0163,  0.0597,\n",
              "         -0.0764,  0.0005, -0.0978,  0.0515,  0.0647,  0.0577,  0.0833, -0.0727,\n",
              "         -0.0418,  0.0009, -0.0625,  0.0561,  0.0708, -0.0413, -0.0961, -0.0522,\n",
              "         -0.0450, -0.0797, -0.0659,  0.0059, -0.0714,  0.0506, -0.0130, -0.0902,\n",
              "         -0.0521,  0.0139, -0.0635, -0.0229, -0.0293,  0.0812,  0.0883, -0.0624,\n",
              "          0.0217, -0.0160,  0.0233, -0.0364,  0.0786,  0.0142, -0.0402, -0.0154,\n",
              "          0.0546,  0.0617, -0.0102, -0.0688,  0.0530,  0.0252, -0.0874,  0.0948,\n",
              "         -0.0877,  0.0868, -0.0701,  0.0867, -0.0858, -0.0356,  0.0920, -0.0557,\n",
              "         -0.0256,  0.0386, -0.0657,  0.0121],\n",
              "        [-0.0857, -0.0506,  0.0164, -0.0848, -0.0594, -0.0231,  0.0687,  0.0023,\n",
              "         -0.0648, -0.0381,  0.0041,  0.0463, -0.0796, -0.0716,  0.0772,  0.0593,\n",
              "         -0.0079, -0.0857, -0.0212,  0.0296,  0.0253, -0.0318, -0.0312,  0.0841,\n",
              "          0.0923,  0.0253, -0.0159,  0.0822,  0.0613, -0.0498, -0.0245,  0.0908,\n",
              "          0.0976, -0.0342,  0.0326,  0.0946,  0.0674, -0.0092,  0.0609, -0.0428,\n",
              "         -0.0634,  0.0783,  0.0089, -0.0475,  0.0305, -0.0721, -0.0390, -0.0417,\n",
              "          0.0846,  0.0750,  0.0391, -0.0735, -0.0719,  0.0329,  0.0794,  0.0865,\n",
              "         -0.0505, -0.0829,  0.0421, -0.0185, -0.0472, -0.0093,  0.0414, -0.0775,\n",
              "         -0.0051,  0.0165,  0.0016, -0.0986,  0.0324,  0.0794,  0.0913,  0.0722,\n",
              "          0.0237,  0.0701,  0.0808,  0.0026,  0.0472,  0.0795,  0.0693, -0.0744,\n",
              "          0.0040, -0.0787, -0.0951,  0.0951,  0.0266,  0.0961,  0.0719, -0.0580,\n",
              "          0.0415, -0.0430, -0.0889,  0.0887, -0.0618,  0.0105,  0.0929, -0.0140,\n",
              "          0.0547, -0.0620, -0.0272,  0.0823]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "zQkjsvH7BRuB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Loss function**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "q6WAiVnzulE7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TqN5BDW-BWb5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training and evaluation"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-ytfpXc5ulE9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def eval_on_test_set():\n",
        "\n",
        "    running_loss=0\n",
        "    num_batches=0    \n",
        "    \n",
        "    correct = 0\n",
        "    \n",
        "    correct_0 = 0\n",
        "    correct_1 = 0\n",
        "    correct_2 = 0\n",
        "    correct_3 = 0\n",
        "    \n",
        "    h = torch.zeros(1, 1, hidden_size)\n",
        "    c = torch.zeros(1, 1, hidden_size)\n",
        "   \n",
        "    h=h.to(device)\n",
        "    c=c.to(device)\n",
        "       \n",
        "    for count in range( len(test_data)) :\n",
        "               \n",
        "        minibatch_data =  test_data[ count  ]\n",
        "        minibatch_data = torch.Tensor(minibatch_data).view(1,-1,3)\n",
        "\n",
        "        minibatch_label = []\n",
        "        for i in range(minibatch_data.shape[1]):\n",
        "            minibatch_label.append(test_label_tensor[ count ])\n",
        "        \n",
        "        #minibatch_label = test_label_tensor[ count  ]\n",
        "        minibatch_label = torch.LongTensor([minibatch_label]).view(-1)\n",
        "        \n",
        "        minibatch_data=minibatch_data.to(device)\n",
        "        minibatch_label=minibatch_label.to(device)\n",
        "                                  \n",
        "        scores = net( minibatch_data, h , c)\n",
        "        \n",
        "        #minibatch_label =   minibatch_label.view(  bs*seq_len ) \n",
        "        #scores          =            scores.view(  bs*seq_len )\n",
        "        \n",
        "        loss = criterion(  scores ,  minibatch_label )    \n",
        "        \n",
        "        h=h.detach()\n",
        "        c=c.detach()\n",
        "            \n",
        "        running_loss += loss.item()\n",
        "        num_batches += 1        \n",
        "        \n",
        "        scores = F.relu(scores)\n",
        "        total_score = torch.sum(scores,0)   \n",
        "        _, predicted = torch.max(total_score,0)\n",
        "        #total += minibatch_label.size(0)\n",
        "        if predicted == minibatch_label[0]:\n",
        "            correct+=1\n",
        "            if predicted == 0:\n",
        "                correct_0+=1\n",
        "            elif predicted == 1:\n",
        "                correct_1+=1\n",
        "            elif predicted == 2:\n",
        "                correct_2+=1                \n",
        "            elif predicted == 3:\n",
        "                correct_3+=1        \n",
        "        #print([predicted, minibatch_label[0], minibatch_label])\n",
        "    \n",
        "    total_loss = running_loss/num_batches \n",
        "    print('test: exp(loss) = ', math.exp(total_loss)  )\n",
        "    print ('correct', correct, 'correcr_0', correct_0,'correct_1',correct_1,'correcr_2', correct_2,'correct_3',correct_3)\n",
        "    print ('Test accuracy:{}%'.format(100 * correct / len(test_data)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e3zJC8lnLDwi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Training by batch of image (not exactly by batch, append in 1 row to avoid uneven tensor shape. )**"
      ]
    },
    {
      "metadata": {
        "id": "l5kaU45LK_pk",
        "colab_type": "code",
        "outputId": "0d9c8362-dd32-40e9-9625-74fd80d2524d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1241
        }
      },
      "cell_type": "code",
      "source": [
        "# change bs above\n",
        "\n",
        "start=time.time()\n",
        "\n",
        "for epoch in range(Epoch):\n",
        "    \n",
        "    # divide the learning rate by 3 except after the first epoch\n",
        "    #if epoch >= 4:\n",
        "    #    lr = lr / 1.25\n",
        "    #else:\n",
        "    #    lr = lr / 2\n",
        "    # create a new optimizer at the beginning of each epoch: give the current learning rate.   \n",
        "    optimizer=torch.optim.Adam( net.parameters())# , lr=lr )\n",
        "        \n",
        "    # set the running quatities to zero at the beginning of the epoch\n",
        "    running_loss=0\n",
        "    num_batches=0    \n",
        "       \n",
        "    # set the initial h and c to be the zero vector\n",
        "    h = torch.zeros( 1, 1, hidden_size)\n",
        "    c = torch.zeros( 1, 1, hidden_size)\n",
        "\n",
        "    # send them to the gpu    \n",
        "    h=h.to(device)\n",
        "    c=c.to(device)\n",
        "    \n",
        "    for count in range(0,len(train_data),bs):\n",
        "        \n",
        "        # Set the gradients to zeros\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # create a minibatch\n",
        "        #minibatch_data = train_data_points[count:count+bs]\n",
        "        \n",
        "        minibatch_data_bs =  train_data[ count:count+bs]\n",
        "        \n",
        "        minibatch_data = []\n",
        "        for i in range(len(minibatch_data_bs)):\n",
        "            for j in range (len(minibatch_data_bs[i])):\n",
        "                minibatch_data.append(minibatch_data_bs[i][j])  \n",
        "        \n",
        "        #print (len(minibatch_data))\n",
        "        \n",
        "        minibatch_data = torch.Tensor(minibatch_data).view(1,-1,3)\n",
        "        \n",
        "        #print(minibatch_data.shape)\n",
        "        #minibatch_data=minibatch_label.view(1,1,-1,200)\n",
        "        #minibatch_label = train_label_tensor[ count ]\n",
        "        #minibatch_label = total_train_label[count:count+bs]\n",
        "        minibatch_label = []\n",
        "        for k in range(len(minibatch_data_bs)):\n",
        "            for l in range(len(minibatch_data_bs[k])):\n",
        "                minibatch_label.append(train_label_tensor[ count+k ])\n",
        "        \n",
        "        #print (minibatch_label)\n",
        "        \n",
        "        #minibatch_label=torch.LongTensor(minibatch_label)\n",
        "        minibatch_label = torch.LongTensor([minibatch_label]).view(-1)\n",
        "\n",
        "        #print (minibatch_label.shape)\n",
        "        # send them to the gpu\n",
        "        minibatch_data=minibatch_data.to(device)\n",
        "        minibatch_label=minibatch_label.to(device)\n",
        "        \n",
        "        #print ('label shape',minibatch_label.shape)\n",
        "        \n",
        "        # Detach to prevent from backpropagating all the way to the beginning\n",
        "        # Then tell Pytorch to start tracking all operations that will be done on h and c\n",
        "        h=h.detach()\n",
        "        c=c.detach()\n",
        "        h=h.requires_grad_()\n",
        "        c=c.requires_grad_()\n",
        "                       \n",
        "        # forward the minibatch through the net        \n",
        "        scores = net( minibatch_data, h , c )\n",
        "        \n",
        "        #print (scores[0])\n",
        "        \n",
        "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
        "        #scores          =            scores.view(  bs*seq_len )  \n",
        "        #minibatch_label =   minibatch_label.view(  bs*seq_len )    \n",
        "        \n",
        "        # Compute the average of the losses of the data points in this huge batch\n",
        "        loss = criterion(  scores ,  minibatch_label )\n",
        "        \n",
        "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
        "        loss.backward()\n",
        "\n",
        "       \n",
        "        optimizer.step()\n",
        "        \n",
        "            \n",
        "        # update the running loss  \n",
        "        running_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "        \n",
        "        \n",
        "    # compute stats for the full training set\n",
        "    total_loss = running_loss/num_batches\n",
        "    elapsed = time.time()-start\n",
        "    \n",
        "    print('')\n",
        "    print('epoch=',epoch, '\\t time=', elapsed, '\\t exp(loss)=',  math.exp(total_loss))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        print('lr= ',param_group['lr'])\n",
        "        \n",
        "    eval_on_test_set()\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch= 0 \t time= 979.8805589675903 \t exp(loss)= 2.439884681668551\n",
            "lr=  0.001\n",
            "test: exp(loss) =  2.457905613544755\n",
            "correct 778 correcr_0 308 correct_1 120 correcr_2 103 correct_3 247\n",
            "Test accuracy:77.8%\n",
            "\n",
            "epoch= 1 \t time= 2044.5767273902893 \t exp(loss)= 2.317037843841241\n",
            "lr=  0.001\n",
            "test: exp(loss) =  2.3779727881721735\n",
            "correct 786 correcr_0 321 correct_1 115 correcr_2 114 correct_3 236\n",
            "Test accuracy:78.6%\n",
            "\n",
            "epoch= 2 \t time= 3103.958749771118 \t exp(loss)= 2.2290939398683784\n",
            "lr=  0.001\n",
            "test: exp(loss) =  2.1797200017752725\n",
            "correct 830 correcr_0 329 correct_1 125 correcr_2 104 correct_3 272\n",
            "Test accuracy:83.0%\n",
            "\n",
            "epoch= 3 \t time= 4157.154712200165 \t exp(loss)= 2.1520667437623873\n",
            "lr=  0.001\n",
            "test: exp(loss) =  2.164291319625073\n",
            "correct 828 correcr_0 325 correct_1 122 correcr_2 110 correct_3 271\n",
            "Test accuracy:82.8%\n",
            "\n",
            "epoch= 4 \t time= 5215.088087797165 \t exp(loss)= 2.0861007336657127\n",
            "lr=  0.001\n",
            "test: exp(loss) =  2.100139221686308\n",
            "correct 842 correcr_0 333 correct_1 133 correcr_2 109 correct_3 267\n",
            "Test accuracy:84.2%\n",
            "\n",
            "epoch= 5 \t time= 6273.7917993068695 \t exp(loss)= 2.022342481019676\n",
            "lr=  0.001\n",
            "test: exp(loss) =  2.077327099389848\n",
            "correct 847 correcr_0 339 correct_1 138 correcr_2 105 correct_3 265\n",
            "Test accuracy:84.7%\n",
            "\n",
            "epoch= 6 \t time= 7336.215461969376 \t exp(loss)= 1.9605609624202642\n",
            "lr=  0.001\n",
            "test: exp(loss) =  2.062638126267677\n",
            "correct 841 correcr_0 334 correct_1 139 correcr_2 108 correct_3 260\n",
            "Test accuracy:84.1%\n",
            "\n",
            "epoch= 7 \t time= 8391.522764921188 \t exp(loss)= 1.9146456551782145\n",
            "lr=  0.001\n",
            "test: exp(loss) =  2.0219480932644824\n",
            "correct 856 correcr_0 335 correct_1 135 correcr_2 107 correct_3 279\n",
            "Test accuracy:85.6%\n",
            "\n",
            "epoch= 8 \t time= 9449.62838935852 \t exp(loss)= 1.884453345351005\n",
            "lr=  0.001\n",
            "test: exp(loss) =  1.9981070776873822\n",
            "correct 869 correcr_0 325 correct_1 137 correcr_2 110 correct_3 297\n",
            "Test accuracy:86.9%\n",
            "\n",
            "epoch= 9 \t time= 10509.151239156723 \t exp(loss)= 1.8513275423807514\n",
            "lr=  0.001\n",
            "test: exp(loss) =  1.9723622010257191\n",
            "correct 875 correcr_0 337 correct_1 128 correcr_2 112 correct_3 298\n",
            "Test accuracy:87.5%\n",
            "\n",
            "epoch= 10 \t time= 11580.743819236755 \t exp(loss)= 1.7773310594614717\n",
            "lr=  0.001\n",
            "test: exp(loss) =  1.9959762410048183\n",
            "correct 868 correcr_0 330 correct_1 129 correcr_2 114 correct_3 295\n",
            "Test accuracy:86.8%\n",
            "\n",
            "epoch= 11 \t time= 12640.373938083649 \t exp(loss)= 1.7554225361921991\n",
            "lr=  0.001\n",
            "test: exp(loss) =  1.9498210771890343\n",
            "correct 871 correcr_0 336 correct_1 124 correcr_2 112 correct_3 299\n",
            "Test accuracy:87.1%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zkaNR3VhBoPw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ]
    },
    {
      "metadata": {
        "id": "WrJBuDPgmcm5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test = torch.randn(2,2)\n",
        "print(test.data)\n",
        "test = F.relu(test)\n",
        "print(test)\n",
        "total =torch.sum(test, 0)\n",
        "\n",
        "print (total)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "iUSEHM3XVMOH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print (train_label_tensor[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fgeN558vulFD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename = 'short_kanji.npz'\n",
        "load_data = np.load(filename,encoding = 'latin1')\n",
        "train_set = load_data['train']\n",
        "valid_set = load_data['valid']\n",
        "test_set = load_data['test']\n",
        "print(type(train_set))\n",
        "print(train_set.dtype)\n",
        "print(train_set.shape)\n",
        "print(random.choice(train_set))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o02U9T9NDWcE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Histories"
      ]
    },
    {
      "metadata": {
        "id": "5ImFwAMNKpFc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " **(56.76 with 2 class dataset) Training by single image**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ApWfPoXvulE_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "start=time.time()\n",
        "\n",
        "for epoch in range(Epoch):\n",
        "    \n",
        "    # divide the learning rate by 3 except after the first epoch\n",
        "    if epoch >= 2:\n",
        "        lr = lr / 3\n",
        "    \n",
        "    # create a new optimizer at the beginning of each epoch: give the current learning rate.   \n",
        "    optimizer=torch.optim.SGD( net.parameters() , lr=lr )\n",
        "        \n",
        "    # set the running quatities to zero at the beginning of the epoch\n",
        "    running_loss=0\n",
        "    num_batches=0    \n",
        "       \n",
        "    # set the initial h and c to be the zero vector\n",
        "    h = torch.zeros( 1, bs, hidden_size)\n",
        "    c = torch.zeros( 1, bs, hidden_size)\n",
        "\n",
        "    # send them to the gpu    \n",
        "    h=h.to(device)\n",
        "    c=c.to(device)\n",
        "    \n",
        "    for count in range( len(train_data)):\n",
        "        \n",
        "        # Set the gradients to zeros\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # create a minibatch\n",
        "        minibatch_data =  train_data[ count]\n",
        "        \n",
        "        \n",
        "        minibatch_data = torch.Tensor(minibatch_data).view(1,-1,2)\n",
        "        \n",
        "        #print(minibatch_data.shape)\n",
        "        #minibatch_data=minibatch_label.view(1,1,-1,200)\n",
        "        #minibatch_label = train_label_tensor[ count ]\n",
        "        minibatch_label = []\n",
        "        for i in range(minibatch_data.shape[1]):\n",
        "            minibatch_label.append(train_label_tensor[ count ])\n",
        "        \n",
        "        #print (minibatch_label)\n",
        "        \n",
        "        #minibatch_label=torch.LongTensor(minibatch_label)\n",
        "        minibatch_label = torch.LongTensor([minibatch_label]).view(-1)\n",
        "\n",
        "        #print (minibatch_label)\n",
        "        # send them to the gpu\n",
        "        minibatch_data=minibatch_data.to(device)\n",
        "        minibatch_label=minibatch_label.to(device)\n",
        "        \n",
        "        #print ('label shape',minibatch_label.shape)\n",
        "        \n",
        "        # Detach to prevent from backpropagating all the way to the beginning\n",
        "        # Then tell Pytorch to start tracking all operations that will be done on h and c\n",
        "        h=h.detach()\n",
        "        c=c.detach()\n",
        "        h=h.requires_grad_()\n",
        "        c=c.requires_grad_()\n",
        "                       \n",
        "        # forward the minibatch through the net        \n",
        "        scores = net( minibatch_data, h , c )\n",
        "        \n",
        "        #print (scores[0])\n",
        "        \n",
        "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
        "        #scores          =            scores.view(  bs*seq_len )  \n",
        "        #minibatch_label =   minibatch_label.view(  bs*seq_len )    \n",
        "        \n",
        "        # Compute the average of the losses of the data points in this huge batch\n",
        "        loss = criterion(  scores ,  minibatch_label )\n",
        "        \n",
        "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
        "        loss.backward()\n",
        "\n",
        "       \n",
        "        optimizer.step()\n",
        "        \n",
        "            \n",
        "        # update the running loss  \n",
        "        running_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "        \n",
        "        \n",
        "    # compute stats for the full training set\n",
        "    total_loss = running_loss/num_batches\n",
        "    elapsed = time.time()-start\n",
        "    \n",
        "    print('')\n",
        "    print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', lr, '\\t exp(loss)=',  math.exp(total_loss))\n",
        "    \n",
        "    eval_on_test_set()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}