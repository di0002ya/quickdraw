{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline - LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\"\"\"\n",
    "Author: ZHANG Yu\n",
    "\n",
    "The code is used to do calsssification of quickdraw dataset using LSTM. \n",
    "The data used here has been dealt with by generate_data.py\n",
    "\n",
    "The simplified process is:\n",
    "1. Get train data and test data as well as their label\n",
    "2. Zero padding the data to the same length\n",
    "3. Choose hyperparameters\n",
    "4. Construct and build the LSTM network\n",
    "5. Train the network\n",
    "6. Evaluate the network using test data\n",
    "\n",
    "To run the code:\n",
    "1. Download quick_draw_output file\n",
    "2. Change path and parameters\n",
    "3. Move away '#' before the code containing 'device' if you want to run on gpu\n",
    "4. Run the code\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os.path as path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### maybe use GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device= torch.device(\"cuda\")\n",
    "#device= torch.device(\"cpu\")\n",
    "#print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "### Prepare train data and test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path='C:/Users/YU007/CE7454_2018/project/quick_draw_output' # change path here\n",
    "\n",
    "with open(path.join(data_path,'data_X'),'rb') as f:\n",
    "    X=pickle.load(f)\n",
    "\n",
    "with open(path.join(data_path,'data_Y_int'),'rb') as f:\n",
    "    Y=pickle.load(f)\n",
    "\n",
    "len_train_X=int(len(X)*0.8)\n",
    "\n",
    "\n",
    "\n",
    "train_X=np.array(X[:len_train_X]) # data\n",
    "train_Y=np.array(Y[:len_train_X]) # label\n",
    "test_X=np.array(X[len_train_X:])\n",
    "test_Y=np.array(Y[len_train_X:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zero padding train data to the same length\n",
    "\n",
    "* final train data: train_Xdata_tensor\n",
    "* train label: train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max stroke number in an image = 47 \n",
      " max point number in a stroke = 123\n",
      "training image number = 3657 \n",
      " testing image number = 915\n"
     ]
    }
   ],
   "source": [
    "stroke_no = np.zeros(len(train_X))\n",
    "point_no = []\n",
    "\n",
    "for i in range(len(train_X)): # number of pictures, ith picture\n",
    "    stroke_no[i] = len(train_X[i])\n",
    "    for j in range(len(train_X[i])): # number of strokes for each picture, jth stroke\n",
    "        point_no.append(len(train_X[i][j][0]))\n",
    "\n",
    "stroke_no_max = int(max(stroke_no))\n",
    "point_no_max = int(max(point_no))\n",
    "\n",
    "print ('max stroke number in an image =',stroke_no_max,'\\n max point number in a stroke =', point_no_max)\n",
    "print ('training image number =', len(train_X),'\\n testing image number =', len(test_X))\n",
    "train_Xdata = np.zeros((len(train_X),stroke_no_max,2,point_no_max))\n",
    "\n",
    "for i in range(len(train_X)):  \n",
    "    for j in range(len(train_X[i])):        \n",
    "        train_Xdata[i][j][0][:len(train_X[i][j][0])]=train_X[i][j][0][:]\n",
    "        train_Xdata[i][j][1][:len(train_X[i][j][0])]=train_X[i][j][1][:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zero padding test data to the same length\n",
    "* final test data: test_Xdata_tensor\n",
    "* test label: test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max stroke number in an image = 32 \n",
      " max point number in a stroke = 66\n",
      "torch.Size([915, 2112, 2])\n"
     ]
    }
   ],
   "source": [
    "stroke_no_test = np.zeros(len(test_X))\n",
    "point_no_test = []\n",
    "\n",
    "for i in range(len(test_X)): # number of pictures, ith picture\n",
    "    stroke_no_test[i] = len(test_X[i])\n",
    "    for j in range(len(test_X[i])): # number of strokes for each picture, jth stroke\n",
    "        point_no_test.append(len(test_X[i][j][0]))\n",
    "\n",
    "stroke_no_maxtest = int(max(stroke_no_test))\n",
    "point_no_maxtest = int(max(point_no_test))\n",
    "\n",
    "print ('max stroke number in an image =',stroke_no_maxtest,'\\n max point number in a stroke =', point_no_maxtest)\n",
    "\n",
    "test_Xdata = np.zeros((len(test_X),stroke_no_maxtest,2,point_no_maxtest))\n",
    "\n",
    "for i in range(len(test_X)):  \n",
    "    for j in range(len(test_X[i])):        \n",
    "        test_Xdata[i][j][0][:len(test_X[i][j][0])]=test_X[i][j][0][:]\n",
    "        test_Xdata[i][j][1][:len(test_X[i][j][0])]=test_X[i][j][1][:]\n",
    "        \n",
    "test_Xdata_tensor = torch.Tensor(test_Xdata)\n",
    "test_Xdata_tensor = test_Xdata_tensor.permute(0, 1, 3, 2)\n",
    "test_Xdata_tensor = test_Xdata_tensor.reshape(len(test_X), stroke_no_maxtest*point_no_maxtest, 2)\n",
    "print (test_Xdata_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 20 # batch size, each batch has n images\n",
    "seq_len = point_no_max * stroke_no_max # 47*123, number of feature points in each image\n",
    "input_size = 2 # number of features,  point\n",
    "hidden_size = 200\n",
    "output_size = 5 # n calsses\n",
    "num_layers = 1 # number of recurrent layers\n",
    "EPOCH = 3 # train the training data n times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a recurrent net class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM_net, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM( input_size , hidden_size , num_layers, batch_first=True  ) # recurrent layer, batch first\n",
    "        self.fc = nn.Linear(    hidden_size , output_size   ) # linear layer\n",
    "\n",
    "        \n",
    "    def forward(self, X, h0, c0 ):\n",
    "        # X shape: bs * seq_len * input_size\n",
    "          \n",
    "        h_seq , _  =   self.lstm( X , (h0, c0) )      # bs*seq_len*hidden_size\n",
    "        out  =   self.fc( h_seq[:, -1, :] )   # bs * output_size, use last feature\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_net(\n",
      "  (lstm): LSTM(2, 200, batch_first=True)\n",
      "  (fc): Linear(in_features=200, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = LSTM_net(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send the weights of the networks to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up manually the weights of the Linear module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the criterion, as well as the following important hyperparameters: \n",
    "* initial learning rate: my_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "my_lr = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate the network on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_on_test_set():\n",
    "    \n",
    "    running_loss=0\n",
    "    num_batches=0  \n",
    "    \n",
    "    correct_no = 0\n",
    "    total_no = 0\n",
    "       \n",
    "    # set the initial h and c to be the zero vector\n",
    "    h = torch.zeros( num_layers, bs, hidden_size)\n",
    "    c = torch.zeros( num_layers, bs, hidden_size)\n",
    "\n",
    "    # send them to the gpu    \n",
    "    # h=h.to(device)\n",
    "    # c=c.to(device)\n",
    "       \n",
    "    for count in range( 0 , len(test_X)-1-bs ,  bs) :\n",
    "               \n",
    "        minibatch_data =  test_Xdata_tensor[ count : count+bs ]\n",
    "        minibatch_label = test_Y[ count : count+bs ]\n",
    "        minibatch_label = torch.LongTensor(minibatch_label)\n",
    "        \n",
    "        # minibatch_data=minibatch_data.to(device)\n",
    "        # minibatch_label=minibatch_label.to(device)\n",
    "                                  \n",
    "        scores  = net( minibatch_data, h , c )\n",
    "         \n",
    "        \n",
    "        loss = criterion(  scores ,  minibatch_label )    \n",
    "        \n",
    "        h=h.detach()\n",
    "        c=c.detach()\n",
    "            \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1        \n",
    "        \n",
    "        _, predicted = torch.max(scores.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    total_loss = running_loss/num_batches \n",
    "    print('test: exp(loss) = ', math.exp(total_loss)  )\n",
    "    print ('Test accuracy:{}%'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do EPOCH passes through the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch= 0 \t time= 3161.2105412483215 \t lr= 1 \t exp(loss)= 4.898336101568618\n",
      "\n",
      "epoch= 1 \t time= 6364.750329017639 \t lr= 1 \t exp(loss)= 4.888860776750387\n",
      "\n",
      "epoch= 2 \t time= 9352.907896518707 \t lr= 0.3333333333333333 \t exp(loss)= 4.842653726983257\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    \n",
    "    # divide the learning rate by 3 except after the first epoch\n",
    "    if epoch >= 2:\n",
    "        my_lr = my_lr / 3\n",
    "    \n",
    "    # create a new optimizer at the beginning of each epoch: give the current learning rate.   \n",
    "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "        \n",
    "    # set the running quatities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "       \n",
    "    # set the initial h and c to be the zero vector\n",
    "    h = torch.zeros( num_layers, bs, hidden_size)\n",
    "    c = torch.zeros( num_layers, bs, hidden_size)\n",
    "\n",
    "    # send them to the gpu    \n",
    "    # h=h.to(device)\n",
    "    # c=c.to(device)\n",
    "    \n",
    "    for count in range( 0 , len(train_X)-1-bs ,  bs):\n",
    "        \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch\n",
    "        minibatch_data =  train_Xdata_tensor[ count : count+bs ] # bs*seq_len*2\n",
    "        minibatch_label = train_Y[ count : count+bs ]    \n",
    "        minibatch_label = torch.LongTensor(minibatch_label)\n",
    "\n",
    "        \n",
    "        # send them to the gpu\n",
    "        # minibatch_data=minibatch_data.to(device)\n",
    "        # minibatch_label=minibatch_label.to(device)\n",
    "        \n",
    "        # Detach to prevent from backpropagating all the way to the beginning\n",
    "        # Then tell Pytorch to start tracking all operations that will be done on h and c\n",
    "        h=h.detach()\n",
    "        c=c.detach()\n",
    "        h=h.requires_grad_()\n",
    "        c=c.requires_grad_()\n",
    "                       \n",
    "        # forward the minibatch through the net        \n",
    "        scores = net( minibatch_data, h , c )\n",
    "        \n",
    "       \n",
    "        \n",
    "        # Compute the average of the losses of the data points in this huge batch\n",
    "        loss = criterion(  scores ,  minibatch_label )\n",
    "        \n",
    "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "        loss.backward()\n",
    "\n",
    "       \n",
    "        optimizer.step()\n",
    "        \n",
    "            \n",
    "        # update the running loss  \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = time.time()-start\n",
    "    \n",
    "    print('')\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))\n",
    "    eval_on_test_set()\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
