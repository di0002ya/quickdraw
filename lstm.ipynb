{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAuthor: ZHANG Yu\\n\\nThe code is used to do calsssification of quickdraw dataset using LSTM. \\nThe data used here has been dealt with by generate_data.py\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Author: ZHANG Yu\n",
    "\n",
    "The code is used to do calsssification of quickdraw dataset using LSTM. \n",
    "The data used here has been dealt with by generate_data.py\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The simplified process is:\n",
    "1. Get train data and test data as well as their label\n",
    "2. Zero padding the data to the same length\n",
    "3. Choose hyperparameters\n",
    "4. Construct and build the LSTM network\n",
    "5. Train the network\n",
    "6. Evaluate the network using test data\n",
    "\n",
    "### To run the code:\n",
    "1. Download quick_draw_output file\n",
    "2. Change path and parameters\n",
    "3. Move away '#' before the code containing 'device' if you want to run on gpu\n",
    "4. Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os.path as path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### maybe use GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device= torch.device(\"cuda\")\n",
    "#device= torch.device(\"cpu\")\n",
    "#print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "### Prepare train data and test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path='C:/Users/YU007/CE7454_2018/project/quick_draw_output' # change path here\n",
    "\n",
    "with open(path.join(data_path,'data_X'),'rb') as f:\n",
    "    X=pickle.load(f)\n",
    "\n",
    "with open(path.join(data_path,'data_Y_int'),'rb') as f:\n",
    "    Y=pickle.load(f)\n",
    "\n",
    "len_train_X=int(len(X)*0.8) # traindata : test data = 4:1\n",
    "\n",
    "\n",
    "\n",
    "train_X=np.array(X[:len_train_X]) # data\n",
    "train_Y=np.array(Y[:len_train_X]) # label\n",
    "test_X=np.array(X[len_train_X:])\n",
    "test_Y=np.array(Y[len_train_X:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_convert(Xdata):\n",
    "\n",
    "    point = []\n",
    "    stroke = []\n",
    "    image = []\n",
    "    data = []\n",
    "    for i in range(len(Xdata)): # number of pictures, ith picture\n",
    "        z = 0\n",
    "        for j in range(len(Xdata[i])): # number of strokes for each picture, jth stroke\n",
    "            \n",
    "            x = 0\n",
    "            y = 0\n",
    "           \n",
    "        \n",
    "            for k in range(len(Xdata[i][j][0])):\n",
    "                x = float(Xdata[i][j][0][k])\n",
    "                y = float(Xdata[i][j][1][k])\n",
    "                point.append([x, y])\n",
    "                \n",
    "        \n",
    "            \n",
    "        data.append([point])\n",
    "        point = []\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train data\n",
    "\n",
    "* final train data: train_data\n",
    "* train label: train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training image number = 3657\n"
     ]
    }
   ],
   "source": [
    "train_data = data_convert(train_X)\n",
    "train_label = train_Y\n",
    "print ('training image number =', len(train_X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data\n",
    "* final test data: test_data\n",
    "* test label: test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing image number = 915\n"
     ]
    }
   ],
   "source": [
    "test_data = data_convert(test_X)\n",
    "test_label = test_Y\n",
    "\n",
    "print ('testing image number =', len(test_X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1 # batch size, each batch has n images\n",
    "#seq_len = point_no_max * stroke_no_max # 47*123, number of feature points in each image\n",
    "input_size = 2 # number of features,  point\n",
    "hidden_size = 200\n",
    "output_size = 5 # n calsses\n",
    "num_layers = 1 # number of recurrent layers\n",
    "EPOCH = 8 # train the training data n times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a recurrent net class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM_net, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM( input_size , hidden_size , num_layers, batch_first=True  ) # recurrent layer, batch first\n",
    "        self.fc = nn.Linear(    hidden_size , output_size   ) # linear layer\n",
    "\n",
    "        \n",
    "    def forward(self, X, h0, c0 ):\n",
    "        # X shape: bs * seq_len * input_size\n",
    "          \n",
    "        h_seq , _  =   self.lstm( X , (h0, c0) )      # bs*seq_len*hidden_size\n",
    "        out  =   self.fc( h_seq[:, -1, :] )   # bs * output_size, use last feature\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_net(\n",
      "  (lstm): LSTM(2, 200, batch_first=True)\n",
      "  (fc): Linear(in_features=200, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = LSTM_net(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send the weights of the networks to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up manually the weights of the Linear module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the criterion, as well as the following important hyperparameters: \n",
    "* initial learning rate: my_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "my_lr = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate the network on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_on_test_set():\n",
    "    \n",
    "    running_loss=0\n",
    "    num_batches=0  \n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "       \n",
    "    # set the initial h and c to be the zero vector\n",
    "    h = torch.zeros( num_layers, bs, hidden_size)\n",
    "    c = torch.zeros( num_layers, bs, hidden_size)\n",
    "\n",
    "    # send them to the gpu    \n",
    "    # h=h.to(device)\n",
    "    # c=c.to(device) len(test_X)\n",
    "    \n",
    "    for count in range( 50) :\n",
    "               \n",
    "        minibatch_data =  test_data[ count ] # bs*seq_len*2\n",
    "        minibatch_data = torch.Tensor(minibatch_data)\n",
    "        minibatch_label = test_label[ count]    \n",
    "        minibatch_label = torch.tensor([minibatch_label.item()])\n",
    "        \n",
    "        # minibatch_data=minibatch_data.to(device)\n",
    "        # minibatch_label=minibatch_label.to(device)\n",
    "                                  \n",
    "        scores  = net( minibatch_data, h , c )\n",
    "         \n",
    "        \n",
    "        loss = criterion(  scores ,  minibatch_label )    \n",
    "        \n",
    "        h=h.detach()\n",
    "        c=c.detach()\n",
    "            \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1        \n",
    "        \n",
    "        _, predicted = torch.max(scores.data, 1)\n",
    "        total += minibatch_label.size(0)\n",
    "        correct += (predicted == minibatch_label).sum().item()\n",
    "    \n",
    "    total_loss = running_loss/num_batches \n",
    "    print('test: exp(loss) = ', math.exp(total_loss)  )\n",
    "    print ('Test accuracy:{}%'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do EPOCH passes through the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch= 0 \t time= 5.373099088668823 \t lr= 5 \t exp(loss)= 3.879211123449398e+27\n",
      "test: exp(loss) =  1.4332634818086385e+24\n",
      "Test accuracy:24.0%\n",
      "\n",
      "epoch= 1 \t time= 12.796756982803345 \t lr= 5 \t exp(loss)= 6.653434829159341e+21\n",
      "test: exp(loss) =  4.783384357049398e+27\n",
      "Test accuracy:24.0%\n",
      "\n",
      "epoch= 2 \t time= 20.453410625457764 \t lr= 1.6666666666666667 \t exp(loss)= 429465661.6957357\n",
      "test: exp(loss) =  6340189.552171972\n",
      "Test accuracy:24.0%\n",
      "\n",
      "epoch= 3 \t time= 29.54923677444458 \t lr= 0.5555555555555556 \t exp(loss)= 7416.534777913035\n",
      "test: exp(loss) =  26113.835449791568\n",
      "Test accuracy:24.0%\n",
      "\n",
      "epoch= 4 \t time= 42.730584383010864 \t lr= 0.1851851851851852 \t exp(loss)= 280.6540479000455\n",
      "test: exp(loss) =  3787.407860818083\n",
      "Test accuracy:6.0%\n",
      "\n",
      "epoch= 5 \t time= 51.02922606468201 \t lr= 0.0617283950617284 \t exp(loss)= 107.03575011306961\n",
      "test: exp(loss) =  1555.0556400832231\n",
      "Test accuracy:20.0%\n",
      "\n",
      "epoch= 6 \t time= 60.394447565078735 \t lr= 0.0205761316872428 \t exp(loss)= 81.82388563963734\n",
      "test: exp(loss) =  1274.5548977374333\n",
      "Test accuracy:30.0%\n",
      "\n",
      "epoch= 7 \t time= 78.03011012077332 \t lr= 0.006858710562414266 \t exp(loss)= 74.44948949299632\n",
      "test: exp(loss) =  1219.7810202521284\n",
      "Test accuracy:30.0%\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    \n",
    "    # divide the learning rate by 3 except after the first epoch\n",
    "    if epoch >= 2:\n",
    "        my_lr = my_lr / 3\n",
    "    \n",
    "    # create a new optimizer at the beginning of each epoch: give the current learning rate.   \n",
    "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "        \n",
    "    # set the running quatities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "       \n",
    "    # set the initial h and c to be the zero vector\n",
    "    h = torch.zeros( num_layers, bs, hidden_size)\n",
    "    c = torch.zeros( num_layers, bs, hidden_size)\n",
    "\n",
    "    # send them to the gpu    \n",
    "    # h=h.to(device)\n",
    "    # c=c.to(device) len(train_X)\n",
    "    \n",
    "    for count in range(200):\n",
    "        \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch\n",
    "        minibatch_data =  train_data[ count ] # bs*seq_len*2\n",
    "        minibatch_data = torch.Tensor(minibatch_data)\n",
    "        minibatch_label = train_label[ count]    \n",
    "        minibatch_label = torch.tensor([minibatch_label.item()])\n",
    "\n",
    "        \n",
    "        # send them to the gpu\n",
    "        # minibatch_data=minibatch_data.to(device)\n",
    "        # minibatch_label=minibatch_label.to(device)\n",
    "        \n",
    "        # Detach to prevent from backpropagating all the way to the beginning\n",
    "        # Then tell Pytorch to start tracking all operations that will be done on h and c\n",
    "        h=h.detach()\n",
    "        c=c.detach()\n",
    "        h=h.requires_grad_()\n",
    "        c=c.requires_grad_()\n",
    "                       \n",
    "        # forward the minibatch through the net        \n",
    "        scores = net( minibatch_data, h , c )\n",
    "       \n",
    "       \n",
    "        # Compute the average of the losses of the data points in this huge batch\n",
    "        loss = criterion(  scores ,  minibatch_label )\n",
    "        \n",
    "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "        loss.backward()\n",
    "\n",
    "       \n",
    "        optimizer.step()\n",
    "        \n",
    "            \n",
    "        # update the running loss  \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = time.time()-start\n",
    "    \n",
    "    print('')\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))\n",
    "    \n",
    "    eval_on_test_set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
